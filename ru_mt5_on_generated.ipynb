{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771d4f40",
   "metadata": {
    "cellId": "xh9jh6kw5ka4dns4pmqm",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.7/site-packages (4.9.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
      "Requirement already satisfied: pympi-ling in /home/jupyter/.local/lib/python3.7/site-packages (1.70.2)\n",
      "Requirement already satisfied: razdel in /home/jupyter/.local/lib/python3.7/site-packages (0.5.0)\n",
      "Requirement already satisfied: importlib-metadata in /kernel/lib/python3.7/site-packages (from transformers) (4.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/jupyter/.local/lib/python3.7/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.7/site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2021.7.6)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/jupyter/.local/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: packaging in /kernel/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /kernel/lib/python3.7/site-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: typing-extensions in /kernel/lib/python3.7/site-packages (from huggingface-hub==0.0.12->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /kernel/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyter/.local/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install transformers sentencepiece pympi-ling razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38535fdd",
   "metadata": {
    "cellId": "hv9hax0wfnjvtq7brmlfd",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 1.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pip-22.0.4\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!/usr/local/bin/python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b9a7992",
   "metadata": {
    "cellId": "5f7mef519mx3q1bp8atsek",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch_lightning\n",
      "  Using cached pytorch_lightning-1.6.0-py3-none-any.whl (582 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/jupyter/.local/lib/python3.7/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /home/jupyter/.local/lib/python3.7/site-packages (from pytorch_lightning) (0.7.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /kernel/lib/python3.7/site-packages (from pytorch_lightning) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /kernel/lib/python3.7/site-packages (from pytorch_lightning) (1.19.4)\n",
      "Requirement already satisfied: torch>=1.8.* in /home/jupyter/.local/lib/python3.7/site-packages (from pytorch_lightning) (1.9.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/jupyter/.local/lib/python3.7/site-packages (from pytorch_lightning) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /kernel/lib/python3.7/site-packages (from pytorch_lightning) (4.1.1)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /home/jupyter/.local/lib/python3.7/site-packages (from pytorch_lightning) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/jupyter/.local/lib/python3.7/site-packages (from pytorch_lightning) (4.49.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2021.7.0)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in /home/jupyter/.local/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.7/site-packages (from packaging>=17.0->pytorch_lightning) (2.4.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.33.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.38.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /kernel/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (51.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata in /kernel/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyter/.local/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/jupyter/.local/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/jupyter/.local/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jupyter/.local/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jupyter/.local/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jupyter/.local/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /kernel/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
      "Installing collected packages: pytorch_lightning\n",
      "Successfully installed pytorch_lightning-1.6.0\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c25401",
   "metadata": {
    "cellId": "8412feundvwplnbvs56cgp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os, sys\n",
    "import re, random\n",
    "import copy\n",
    "import json\n",
    "import pympi\n",
    "import pymystem3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "from pyeaf.pyeaf import EAFReader\n",
    "from pyeaf.text import RSLStemmer, GramBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa781cd",
   "metadata": {
    "cellId": "jh5t3a7cgje3wltljtr3sm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyeaf.pyeaf.EAFReader at 0x7f461dda3850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "#er = EAFReader(directory=\"../Разметки eaf\")\n",
    "#er.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025afbf4",
   "metadata": {
    "cellId": "2tdfk4ryt3sbx6bhls48tb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#sentences_rus = er.get_sentences(er.RUS)\n",
    "#sentences_rsl = er.get_sentences(er.RSL_R)\n",
    "\n",
    "# sentences_rsl_left = er.get_sentences(er.RSL_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5c0ffc3",
   "metadata": {
    "cellId": "mvchnh9xspbrj5w17hzdho"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#stem_sentences_rsl = list(map(RSLStemmer.stem_sentence, sentences_rsl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c219df",
   "metadata": {
    "cellId": "ocuil4cb2fh97uyuu6msg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import textwrap\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23359bd3",
   "metadata": {
    "cellId": "uskb97ili1pkgirevau5n"
   },
   "source": [
    "Russian language should be tokenized the same way it did in the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81f2a0c",
   "metadata": {
    "cellId": "0z7kul2lo1sh3lsygskvd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3835, 817, 264, 832, 5912, 310, 7451, 637, 5640, 324, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "from transformers import T5Tokenizer, MT5Model, AdamW\n",
    "\n",
    "# load model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"cointegrated/rut5-small\")\n",
    "\n",
    "# test\n",
    "tokenizer('Какой-то текст на русском языке.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5208e320",
   "metadata": {
    "cellId": "0xnh4o9n2ouk8l8qsrqkv5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# don't mind me just looking\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2049517",
   "metadata": {
    "cellId": "72qxenor8nses4pqagjopw"
   },
   "source": [
    "Теперь мы можем настроить логгирование и загрузить модельку для русского вот отсюда:\n",
    "\n",
    "https://huggingface.co/cointegrated/rut5-small\n",
    "\n",
    "Я беру именно эту, потому что там только русский язык и моделька маленькая, значит, моя память это потянет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f4cd7da",
   "metadata": {
    "cellId": "v2o5kdclpbh3x42oxojn",
    "execution_id": "9666d7e8-592a-470c-ab91-f9f165276d12"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import logging\n",
    "from transformers import MT5ForConditionalGeneration\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"cointegrated/rut5-small\", return_dict=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ea67f",
   "metadata": {
    "cellId": "1yniams634x89uwbhu2k5g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3302d76",
   "metadata": {
    "cellId": "8jxzdgh9mvt8qlsjp1fhp9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5Config {\n",
       "  \"_name_or_path\": \"cointegrated/rut5-small\",\n",
       "  \"architectures\": [\n",
       "    \"MT5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 1024,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"mt5\",\n",
       "  \"num_decoder_layers\": 8,\n",
       "  \"num_heads\": 6,\n",
       "  \"num_layers\": 8,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"T5Tokenizer\",\n",
       "  \"transformers_version\": \"4.9.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 20100\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15320e6",
   "metadata": {
    "cellId": "nrae3i84dm80lqhjrai9cpm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "input_ids_test = tokenizer(\n",
    "    \"Что тут вообще происходит?\",\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids.to(device)\n",
    "\n",
    "generated_ids = model.generate(input_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7168bfc4",
   "metadata": {
    "cellId": "6mblx9047xk5xt5tlcb6ij"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Что тут вообще происходит вообще</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "preds = [\n",
    "    tokenizer.decode(input_id)\n",
    "    for input_id in generated_ids\n",
    "]\n",
    "\n",
    "' '.join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facce493",
   "metadata": {
    "cellId": "pvpi2y1t2w8pjvj2kbo99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эхал Грека поблизилась от реки, видит стела неизвестный греки в Раком.\n",
      "Грека в реке Лехаре увидела смертника грекой по иску от Рака, видит Ехал Греку.\n",
      "Ехал Грека после расследование крушения у Трампа и видит греку рак в реке.\n",
      "Грека и Ехал греки под рекой сошла на место, видит Траку в проливе Рак.\n",
      "Грека и Ехал греко, видевшие погибшую в Ракке острова Трампа через реку вылетел в древнюю\n",
      "Грека на борту через реку, видит греков в реке рак, замечает Ехал Хурга.\n",
      "Грека выбросит Ехал Грек через реку, видят Авра и врач Афганистана.\n",
      "Грека в реке, которая перебросила Рим через реку подряд видит греко на рак.\n",
      "Ехал Грека для проведения корабля у отрезания в реку Рима нашли рак по \"кору\" с Афганистана\n",
      "Грека приводит в реке рак, видит погибшую грек во время столкновения с Пелом.\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# don't mind me just checking\n",
    "text = 'Ехал Грека через реку, видит Грека в реке рак. '\n",
    "inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    hypotheses = model.generate(\n",
    "        **inputs, \n",
    "        do_sample=True, top_p=0.95, num_return_sequences=10, \n",
    "        repetition_penalty=2.5,\n",
    "        max_length=32,\n",
    "    )\n",
    "for h in hypotheses:\n",
    "    print(tokenizer.decode(h, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf46dea3",
   "metadata": {
    "cellId": "q7ozk3wj85d3xltu9b2zf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[790, 1578, 268, 259, 737, 263, 3060, 687, 541, 1, 790, 1578, 268, 259, 286, 263, 280, 3060, 687, 541, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "batch_sentences_ru = [[\"hello\", \"rus\", \"sentence\"]]\n",
    "batch_sentences_rsl = [[\"hello\", \"rsl\", \"sentence\"]]\n",
    "encoded_inputs = tokenizer(batch_sentences_ru,\n",
    "                           batch_sentences_rsl,\n",
    "                           is_split_into_words=True)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c957660",
   "metadata": {
    "cellId": "u0x1ysii7ekpwsn4oyriy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello rus sentence</s> hello rsl sentence</s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "preds = [\n",
    "    tokenizer.decode(input_id)\n",
    "    for input_id in encoded_inputs['input_ids']\n",
    "]\n",
    "\n",
    "' '.join(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9fcfd",
   "metadata": {
    "cellId": "yb0m84fe1xvn6ju22psqo"
   },
   "source": [
    "### Подгрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ef9263",
   "metadata": {
    "cellId": "93zdkla8trhndintkztuad"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0a6c385738f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generated_stemed.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stem_rus'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_stem_sent_rus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[]\\''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\', \\''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rsl'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_stem_rsl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[]\\''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\', \\''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gram_rus'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_gram_sent_rus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "generated_data = pd.read_csv('generated_stemed.csv', encoding='utf-8')\n",
    "\n",
    "generated_data['stem_rus'] = generated_data['new_stem_sent_rus'].apply(lambda sent: sent.strip('[]\\'').split('\\', \\''))\n",
    "generated_data['rsl'] = generated_data['new_stem_rsl'].apply(lambda sent: sent.strip('[]\\'').split('\\', \\''))\n",
    "generated_data['gram_rus'] = generated_data['new_gram_sent_rus'].apply(lambda sent: json.loads(sent.replace('\\'', '\\\"')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d712e",
   "metadata": {
    "cellId": "3z028m1lxlbn342uwl1kue"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "test_data = pd.read_csv('test_data.csv', encoding='utf-8')\n",
    "\n",
    "test_data['stem_rus'] = test_data['test_stem_rus'].apply(lambda sent: sent.strip('[]\\'').split('\\', \\''))\n",
    "test_data['rsl'] = test_data['test_rsl'].apply(lambda sent: sent.strip('[]\\'').split('\\', \\''))\n",
    "test_data['gram_rus'] = test_data['test_gram_rus'].apply(lambda sent: json.loads(sent.replace('\\'', '\\\"')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a010e1",
   "metadata": {
    "cellId": "h45mupphzc0yaklx4wnry"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "generated_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fea760",
   "metadata": {
    "cellId": "8qsozq4vdki0bnf4s9mml8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Preprocess:\n",
    "    def __init__(self, tokenizer, max_length = 396):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    #def list_of_lists(self, data_col):\n",
    "    #    return list(map(list, data_col))\n",
    "    \n",
    "    def tokenization(self, batch):\n",
    "        #mapped_rus = self.list_of_lists(batch['stem_rus'])\n",
    "        #mapped_rsl = self.list_of_lists(batch['rsl'])\n",
    "        model_inputs = self.tokenizer(batch['stem_rus'],\n",
    "                                      max_length=self.max_length,\n",
    "                                      padding=\"max_length\",\n",
    "                                      is_split_into_words=True,\n",
    "                                      truncation=\"longest_first\",\n",
    "                                      add_special_tokens=True,\n",
    "                                      return_tensors=\"pt\")\n",
    "        ## Setup the tokenizer for targets\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(batch['rsl'],\n",
    "                                    padding=\"max_length\",\n",
    "                                    truncation=\"longest_first\",\n",
    "                                    max_length=self.max_length,\n",
    "                                    add_special_tokens=True,\n",
    "                                    is_split_into_words=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "# batched=True, batch_size=None ?\n",
    "\n",
    "p = Preprocess(tokenizer, max_length=396)\n",
    "#train_dataset_encoded = p.tokenization(generated_data)\n",
    "train_dataset_encoded = p.tokenization(test_data.iloc[0])  # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdbb76e3",
   "metadata": {
    "cellId": "kkmve2spi8m35zfbqq1xog"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_stem_rus    ['а', 'когда', 'дед', 'мороз', 'вернуться', 'д...\n",
       "test_gram_rus    [['CONJ'], ['CONJ'], ['им', 'мн', 'S', 'муж', ...\n",
       "test_rsl         ['indx', 'дед.мороз', 'clf', 'clf.группа', 'cl...\n",
       "stem_rus         [а, когда, дед, мороз, вернуться, домой, мален...\n",
       "rsl              [indx, дед.мороз, clf, clf.группа, clf.перемес...\n",
       "gram_rus         [[CONJ], [CONJ], [им, мн, S, муж, од], [им, мн...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "test_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61742fe",
   "metadata": {
    "cellId": "5g5wv2mx6poz0jnl3xk0h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# test \n",
    "tokenizer.decode(train_dataset_encoded['labels'].squeeze()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd4582a",
   "metadata": {
    "cellId": "be34ai75lwur519lxxpuus"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_dataset_encoded['input_ids'].shape[0] #[0], test_data['rsl'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2404649",
   "metadata": {
    "cellId": "ezol8eglqpp8bxl57hix9b"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "test_dataset_encoded = p.tokenization(test_data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca870e4a",
   "metadata": {
    "cellId": "dkvstpy4kk82l5nnagacjr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_dataset_encoded.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da48c0",
   "metadata": {
    "cellId": "65k28l2pwg9hqh80cou9ms"
   },
   "source": [
    "> labels (torch.LongTensor of shape (batch_size,), optional) – Labels for computing the sequence classification/regression loss. Indices should be in [-100, 0, ..., config.vocab_size - 1]. All labels set to -100 are ignored (masked), the loss is only computed for labels in [0, ..., config.vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4de1e970",
   "metadata": {
    "cellId": "1bawlvxgj254tlasid3pc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# del padding indeces from forward function\n",
    "test_dataset_encoded['labels'][test_dataset_encoded['labels'] == 0] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17387fe5",
   "metadata": {
    "cellId": "4m61j6hc3gg6tn390hcop"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# del padding indeces from forward function\n",
    "train_dataset_encoded['labels'][train_dataset_encoded['labels'] == 0] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "502a3d7f",
   "metadata": {
    "cellId": "y6vm0mbu54anownmfmdxn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4578,   329, 13684,   260, 19625,   317,   280,   367,   317,   280,\n",
       "          367,   260, 11963,   317,   280,   367,   260,  8628,  9522,  1334,\n",
       "         5804,   817,   317,   280,   367,   317,   280,   367,   260, 11963,\n",
       "          317,   280,   367,   260,  8628,  9522,  1334, 13684,   260, 19625,\n",
       "          259,  6821,  1008, 11293,  6948,   805,  6722,  1182, 14518,  5494,\n",
       "         1626,   333,   325,   263,  3933,  6918,  6426,  1358,   317,   280,\n",
       "          367,   317,   280,   367,   260,  8628,  9209,  5021,     1,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "test_dataset_encoded['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc35873",
   "metadata": {
    "cellId": "y8ridpz48f7i7zcyar0rq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class MTDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: T5Tokenizer, max_length: int = 396):\n",
    "        self.p = Preprocess(tokenizer, max_length=max_length)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        #print(self.max_length)\n",
    "        #print(self.tokenizer)\n",
    "        dataset_encoded = self.p.tokenization(data_row)\n",
    "        dataset_encoded['labels'][dataset_encoded['labels'] == 0] = -100\n",
    "        return dict(\n",
    "            #rus=data_row['stem_rus'],\n",
    "            #rsl=data_row['rsl'],\n",
    "            input_ids=dataset_encoded['input_ids'].flatten(),\n",
    "            attention_mask=dataset_encoded['attention_mask'].flatten(),\n",
    "            labels=dataset_encoded['labels'].flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b784d304",
   "metadata": {
    "cellId": "udarip6xk6lc7nk2gg5gk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='cointegrated/rut5-small', vocab_size=20200, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "468ea4c4",
   "metadata": {
    "cellId": "ufu8uj3tlw41zpyod006v"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#train_dataset = MTDataset(generated_data, tokenizer)\n",
    "test_dataset = MTDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99b2d936",
   "metadata": {
    "cellId": "eauvjut7ica1xdc2ce46d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  259,   308,   259,  5188, 13684, 14093,  5484, 11630,  5804,   817,\n",
      "          259,  6821,  1008, 13684, 14093,   805,  6722,   778,  5494,  1626,\n",
      "          259,   279,   388,  6024,   259,   308,   259,   396,  1049,   259,\n",
      "         8052,   315,  6918,  1049,  1108,  6426,  1358, 17091,  6577,     1,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([ 4578,   329, 13684,   260, 19625,   317,   280,   367,   317,   280,\n",
      "          367,   260, 11963,   317,   280,   367,   260,  8628,  9522,  1334,\n",
      "         5804,   817,   317,   280,   367,   317,   280,   367,   260, 11963,\n",
      "          317,   280,   367,   260,  8628,  9522,  1334, 13684,   260, 19625,\n",
      "          259,  6821,  1008, 11293,  6948,   805,  6722,  1182, 14518,  5494,\n",
      "         1626,   333,   325,   263,  3933,  6918,  6426,  1358,   317,   280,\n",
      "          367,   317,   280,   367,   260,  8628,  9209,  5021,     1,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100])}\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "#test\n",
    "for el in test_dataset:\n",
    "    print(el)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562b6277",
   "metadata": {
    "cellId": "5f14wuzy5kgqdp8qfaqz1q"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class MTDataModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        batch_size: int = 16,\n",
    "        max_length: int = 396\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.train_df = train_df\n",
    "        self.test_df = train_df\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def setup(self):\n",
    "        self.train_dataset = MTDataset(self.train_df, self.tokenizer, self.max_length)\n",
    "        self.test_dataset = MTDataset(self.test_df, self.tokenizer, self.max_length)\n",
    "        #for el in self.train_dataset:\n",
    "        #    print(el)\n",
    "        #    break\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        #for el in self.train_dataset:\n",
    "        #    print(el)\n",
    "        #    break\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=8\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=8\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a830fa71",
   "metadata": {
    "cellId": "q4vh73qtdcpa3w4rklpbkp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCH = 3\n",
    "\n",
    "data_module = MTDataModule(generated_data, test_data, tokenizer, batch_size=BATCH_SIZE, max_length=396)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53c51dff",
   "metadata": {
    "cellId": "qz86p7q405nd52pkzaw92k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f96c67562d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9e114b",
   "metadata": {
    "cellId": "o07x7sywqxxo4d6r7yhi",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pillow==8.2.0\n",
      "  Downloading Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.7.0 requires torch==1.6.0, but you have torch 1.9.0 which is incompatible.\n",
      "mmdet 2.3.0rc0+c6b5ca2 requires Pillow<=6.2.2, but you have pillow 8.2.0 which is incompatible.\n",
      "mmdet 2.3.0rc0+c6b5ca2 requires torch==1.6.0, but you have torch 1.9.0 which is incompatible.\n",
      "enot-utils 1.0.2 requires torch==1.6.0, but you have torch 1.9.0 which is incompatible.\n",
      "enot-utils 1.0.2 requires tqdm>=4.50.0, but you have tqdm 4.49.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pillow-8.2.0\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "#%pip install pillow==8.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d13d002",
   "metadata": {
    "cellId": "rlwy4sazinqya58zr6e9rc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class RusRSLModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = MT5ForConditionalGeneration.from_pretrained(\n",
    "            \"cointegrated/rut5-small\", return_dict=True).to(device)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #print(batch_idx)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d4086d3",
   "metadata": {
    "cellId": "t1i46bfw7qfv71th169hvm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = RusRSLModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba3747e2",
   "metadata": {
    "cellId": "sflsvr44o9j2ft2g827gi4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RusRSLModel(\n",
       "  (model): MT5ForConditionalGeneration(\n",
       "    (shared): Embedding(20100, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(20100, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 6)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(20100, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 6)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedGeluDense(\n",
       "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=20100, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84f453",
   "metadata": {
    "cellId": "e64untkfodens3rg5ayzre"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "508ae38a",
   "metadata": {
    "cellId": "ck53ale2mt9085rpsfye"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=N_EPOCH,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1\n",
    "    #callbacks=pl.callbacks.progress.TQDMProgressBar(refresh_rate=30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8058a9bb",
   "metadata": {
    "cellId": "1q7s4ds5q9gt4fv6hc2thb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching TensorBoard...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe id=\"tensorboard-frame-e6453cd3e1d32dfe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "        </iframe>\n",
       "        <script>\n",
       "        function getCookie(name) {\n",
       "          let matches = document.cookie.match(new RegExp(\n",
       "            \"(?:^|; )\" + name.replace(/([\\.$?*|{}\\(\\)\\[\\]\\\\/\\+^])/g, '\\$1') + \"=([^;]*)\"\n",
       "          ));\n",
       "          return matches ? decodeURIComponent(matches[1]) : undefined;\n",
       "        }\n",
       "        \n",
       "        (function() {\n",
       "            const frame = document.getElementById(\"tensorboard-frame-e6453cd3e1d32dfe\");\n",
       "            const apiUrl = new URL(document.location.href.split('?')[0] + '/../api/tensorboard')\n",
       "            const xsrfToken = getCookie('_xsrf');\n",
       "            \n",
       "            fetch(apiUrl, {\n",
       "                method: 'POST',\n",
       "                mode: 'cors',\n",
       "                cache: 'no-cache',\n",
       "                credentials: 'same-origin',\n",
       "                headers: {\n",
       "                  'Content-Type': 'application/json',\n",
       "                  'X-XSRFToken': xsrfToken\n",
       "                },\n",
       "                redirect: 'follow',\n",
       "                referrerPolicy: 'origin', // no-referrer, *no-referrer-when-downgrade, origin, origin-when-cross-origin, same-origin, strict-origin, strict-origin-when-cross-origin, unsafe-url\n",
       "                body: JSON.stringify({\n",
       "                    'logdir': 'logs/fit'\n",
       "                })\n",
       "              }).then(response => {\n",
       "                response.json().then(json => {\n",
       "                    if (json.name != undefined) {\n",
       "                        const tbId = json.name;\n",
       "                        console.debug('tbId = ' + tbId);\n",
       "                        const url = new URL(document.location.href.split('?')[0] + '/../tensorboard/' + tbId);\n",
       "                        frame.src = url;\n",
       "                    }\n",
       "                });\n",
       "              });\n",
       "        })();\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfe50451",
   "metadata": {
    "cellId": "ls497jdz65i2y0u433hcjh"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f518bb2449ea49d1b8a6ab2263c06218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Sanity Checking'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                        | Params\n",
      "------------------------------------------------------\n",
      "0 | model | MT5ForConditionalGeneration | 64.6 M\n",
      "------------------------------------------------------\n",
      "64.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "64.6 M    Total params\n",
      "258.578   Total estimated model params size (MB)\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4764f0b2ff4d48419972a9713b85631f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "\u001b[0;31mKernelNotResponding\u001b[0m",
     "evalue": "The kernel died unexpectedly and has been restarted. If the restart fails, restart the kernel from the Kernel menu.",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "#!g1.1\n",
    "trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74398076",
   "metadata": {
    "cellId": "gh4e1uf3rnun5uqj0tmkl"
   },
   "source": [
    "### Finetuning модели на нагенерированных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a93100f",
   "metadata": {
    "cellId": "8ao5df9bizf8ff573bu39h"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75435a4539344d32812603936015801e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2236.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import sacrebleu\n",
    "from datasets import load_metric \n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "#def compute_metrics(rsl_preds, truth_rsl):\n",
    "#    # TODO: also implement BERT-score\n",
    "#    rus_rsl_bleu = sacrebleu.corpus_bleu(rsl_preds, [truth_rsl])\n",
    "#    return {\"BLEU\": rus_rsl_bleu.score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77326a",
   "metadata": {
    "cellId": "wxxphhcg6gp4t846juu1of"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = RusRSLModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76dcff55",
   "metadata": {
    "cellId": "smu80fozob4gca9vcpghl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = train_dataset_encoded['input_ids'].shape[0] // batch_size\n",
    "model_name = f\"modelT5-finetuned-gen\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_name,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    fp16=False)  # True\n",
    "#push_to_hub=True,\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca27d87d",
   "metadata": {
    "cellId": "nrfdp7u1limj6tdxdva2b"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae8d95c1",
   "metadata": {
    "cellId": "ik1wsf6q5wr2pfzzrj8xqd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">modelT5-finetuned-gen</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/annaklezovich/huggingface\" target=\"_blank\">https://wandb.ai/annaklezovich/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/annaklezovich/huggingface/runs/3ehrzylz\" target=\"_blank\">https://wandb.ai/annaklezovich/huggingface/runs/3ehrzylz</a><br/>\n",
       "                Run data is saved locally in <code>/home/jupyter/work/resources/Notebooks/wandb/run-20220324_080811-3ehrzylz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8f7119045ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             raise KeyError(\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0;34m\"Indexing with integers (to access backend Encoding for a given batch index) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0;34m\"is not available when using Python based tokenizers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mannaklezovich\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:872: UserWarning: The following variables cannot be serialized: trainer\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset_encoded,\n",
    "    eval_dataset=test_dataset_encoded,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "d39d3c5a-b175-47fe-9235-c7ebb2afb1bc",
  "notebookPath": "Notebooks/ru_mt5_on_generated.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "8lxu5z1gu4iy6w3qhk6ol"
   },
   "source": [
    "## Генерация параллельных предложений 2.0\n",
    "\n",
    "В первой тетрадке generate_out_of_existent.ipynb мы заменяли только наречия, а в этой будем заменять:\n",
    "\n",
    "* существительные в именительном падеже\n",
    "* существительные в винительном падеже\n",
    "* переходные глаголы\n",
    "* (непереходные тоже)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "mwvyz854a8iygzvugd0cg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os, sys\n",
    "import re, random\n",
    "import copy\n",
    "import json\n",
    "import pympi\n",
    "import pymystem3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "from pyeaf.pyeaf import EAFReader\n",
    "from pyeaf.text import VocabularyVectorizer, TextStemmer, RSLStemmer, GramBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "83j4afmjmiwb5ernua75s4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  Разметки_eaf-20211027.zip\n",
      "  inflating: Разметки eaf/Паленный_13_КАК.eaf  \n",
      "  inflating: Разметки eaf/Салий_2_ЕСЯ.eaf  \n",
      "  inflating: Разметки eaf/Шец1_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Добрые_истории_5_ААС.eaf  \n",
      "  inflating: Разметки eaf/бальзам_ААС.eaf  \n",
      "  inflating: Разметки eaf/диктант5_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Бедная_девочка_СГБ.eaf  \n",
      "  inflating: Разметки eaf/публикации_АМК.eaf  \n",
      "  inflating: Разметки eaf/Сокол_5_ААС.eaf  \n",
      "  inflating: Разметки eaf/соловей_и_роза_часть_4_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Золушка_сцена_5_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Каша_из_топора_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Золушка_сцена_6_ВПР.eaf  \n",
      "  inflating: Разметки eaf/соловей_и_роза_часть_5_СГБ.eaf  \n",
      "  inflating: Разметки eaf/диктант3_СГБ.eaf  \n",
      "  inflating: Разметки eaf/времена_года_ККД.eaf  \n",
      "  inflating: Разметки eaf/диктант1_СГБ.eaf  \n",
      "  inflating: Разметки eaf/диктант2_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Лев_и_мышь_ВПР.eaf  \n",
      "  inflating: Разметки eaf/зарплату_маме_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Ашуркова_2_АРП.eaf  \n",
      "  inflating: Разметки eaf/золушка_сцена_2_ВПР.eaf  \n",
      "  inflating: Разметки eaf/ковид_ААС.eaf  \n",
      "  inflating: Разметки eaf/маленький_дед_мороз_ККД.eaf  \n",
      "  inflating: Разметки eaf/ПЭ_СГБ.eaf  \n",
      "  inflating: Разметки eaf/энергия_пищи_СГБ.eaf  \n",
      "  inflating: Разметки eaf/золушка_сцена_1_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Салий_4_АМК.eaf  \n",
      "  inflating: Разметки eaf/муж_и_дочь_исчезли_СГБ.eaf  \n",
      "  inflating: Разметки eaf/как удержать счастье_СИШ.eaf  \n",
      "  inflating: Разметки eaf/набор текста_СИШ.eaf  \n",
      "  inflating: Разметки eaf/марафон_диетолога_СГБ.eaf  \n",
      "  inflating: Разметки eaf/остеопатия_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Накопления_КАК.eaf  \n",
      "  inflating: Разметки eaf/Берлизова_1_АВК.eaf  \n",
      "  inflating: Разметки eaf/дпржя_АМК.eaf  \n",
      "  inflating: Разметки eaf/жизнь_на_Кубани_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Попелло_2_АЛС.eaf  \n",
      "  inflating: Разметки eaf/Приходько_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Берлизова_2_АВК.eaf  \n",
      "  inflating: Разметки eaf/Берлизова_5_ДАС.eaf  \n",
      "  inflating: Разметки eaf/Гилберт_КАК.eaf  \n",
      "  inflating: Разметки eaf/Толерантность_1_АЛС.eaf  \n",
      "  inflating: Разметки eaf/похудение_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Попелло_1_АЛС.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_10_ААС.eaf  \n",
      "  inflating: Разметки eaf/Первая_помощь_1_ККД.eaf  \n",
      "  inflating: Разметки eaf/Сокол_6_КАК.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик 2_ААС.eaf  \n",
      "  inflating: Разметки eaf/Паленный_9_КАК.eaf  \n",
      "  inflating: Разметки eaf/Сокол_4_ААС.eaf  \n",
      "  inflating: Разметки eaf/соловей_и_роза_часть_3_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Животные_убийцы_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Сергучев_1_ЕСЯ.eaf  \n",
      "  inflating: Разметки eaf/Паленный_2_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Сказка_про_гусеничку_АЕГ.eaf  \n",
      "  inflating: Разметки eaf/Посессивность_Руфина_СГБ.eaf  \n",
      "  inflating: Разметки eaf/плюшевый_кролик_КАК.eaf  \n",
      "  inflating: Разметки eaf/Лиза_Жидкова_ОСБ.eaf  \n",
      "  inflating: Разметки eaf/Исаев_4_АМК.eaf  \n",
      "  inflating: Разметки eaf/Сокол_7_СГБ.eaf  \n",
      "  inflating: Разметки eaf/слышащие_переводчики_ККД.eaf  \n",
      "  inflating: Разметки eaf/Реклама_ККД.eaf  \n",
      "  inflating: Разметки eaf/Связь_глухих_2_ККД.eaf  \n",
      "  inflating: Разметки eaf/Как_вы_думаете_исчезнет_ли_мир_глухих_ККД.eaf  \n",
      "  inflating: Разметки eaf/Инклюзивное_ККД.eaf  \n",
      "  inflating: Разметки eaf/Чем_связаны_глухие_ККД.eaf  \n",
      "  inflating: Разметки eaf/Домовенок_про_семью_ККД.eaf  \n",
      "  inflating: Разметки eaf/золушка_сцена_3_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Диана_мнение_КАК.eaf  \n",
      "  inflating: Разметки eaf/Кто_входит_в_сообщество_глухих_ККД.eaf  \n",
      "  inflating: Разметки eaf/Финский_поиск_ККД.eaf  \n",
      "  inflating: Разметки eaf/Паленный_11_КАК.eaf  \n",
      "  inflating: Разметки eaf/Паленный_6_КАК.eaf  \n",
      "  inflating: Разметки eaf/Мира_адаптис_КАК.eaf  \n",
      "  inflating: Разметки eaf/Паленный_10_КАК.eaf  \n",
      "  inflating: Разметки eaf/Паленный_7_КАК.eaf  \n",
      "  inflating: Разметки eaf/1_и_9_мая_КАК.eaf  \n",
      "  inflating: Разметки eaf/ПЭ2_СГБ.eaf  \n",
      "  inflating: Разметки eaf/майские_КАК.eaf  \n",
      "  inflating: Разметки eaf/Сокол_2_ААС.eaf  \n",
      "  inflating: Разметки eaf/23_февраля_и_8_марта_ККД.eaf  \n",
      "  inflating: Разметки eaf/надоесть_КАК.eaf  \n",
      "  inflating: Разметки eaf/Исаев_3_АМК.eaf  \n",
      "  inflating: Разметки eaf/ОА_часть_опроса_ККД.eaf  \n",
      "  inflating: Разметки eaf/Беляева_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Кеша1_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Сокол_1_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Кеша2_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Исаев_6_АМК.eaf  \n",
      "  inflating: Разметки eaf/Созависимость_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Море_в_подарок_АЕГ.eaf  \n",
      "  inflating: Разметки eaf/Медвежонок_Ларси_АЕГ.eaf  \n",
      "  inflating: Разметки eaf/График работы ДЦ «Адаптис» в новогодние праздники 2020-2021_ЕСЯ.eaf  \n",
      "  inflating: Разметки eaf/Мира_опыт_КАК.eaf  \n",
      "  inflating: Разметки eaf/Паленный_5_КАК.eaf  \n",
      "  inflating: Разметки eaf/ЕА_Жидкова_2_СГБ.eaf  \n",
      "  inflating: Разметки eaf/тех_проблемы_ККД.eaf  \n",
      "  inflating: Разметки eaf/ЕА_Жидкова_СГБ_ВПР.eaf  \n",
      "  inflating: Разметки eaf/Паленный_4_КАК.eaf  \n",
      "  inflating: Разметки eaf/Гена_Тихенко_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Попелло_3_ЕЮП.eaf  \n",
      "  inflating: Разметки eaf/Настя_мнение_КАК.eaf  \n",
      "  inflating: Разметки eaf/Паленный_12_КАК.eaf  \n",
      "  inflating: Разметки eaf/Шамаева 1_ААС.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_3_ААС.eaf  \n",
      "  inflating: Разметки eaf/Сокол_3_ААС.eaf  \n",
      "  inflating: Разметки eaf/Исаев_2_АМК.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_7_ААС.eaf  \n",
      "  inflating: Разметки eaf/Шамаева 2_ААС.eaf  \n",
      "  inflating: Разметки eaf/3д_слим_КАК.eaf  \n",
      "  inflating: Разметки eaf/Попелло_4_ДАС.eaf  \n",
      "  inflating: Разметки eaf/Паленный_8_КАК.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик 1_ААС.eaf  \n",
      "  inflating: Разметки eaf/Сокол_8_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Сокол_9_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_8_ААС.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_9_ААС.eaf  \n",
      "  inflating: Разметки eaf/МБ_ЕСЯ.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_11_ААС.eaf  \n",
      "  inflating: Разметки eaf/Симонов_СДК.eaf  \n",
      "  inflating: Разметки eaf/Виктория Скворцова_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Исаев_5_АМК.eaf  \n",
      "  inflating: Разметки eaf/Сергучев_2_ЕСЯ.eaf  \n",
      "  inflating: Разметки eaf/Сергей_Синодов_ААС.eaf  \n",
      "  inflating: Разметки eaf/Салий_1_ЕСЯ.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_Я_понимаю_ВАБ.eaf  \n",
      "  inflating: Разметки eaf/Исаев_1_АМК.eaf  \n",
      "  inflating: Разметки eaf/Коновалов_ЕЮП.eaf  \n",
      "  inflating: Разметки eaf/Субботина_Лигмир_ААС.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_5_ААС.eaf  \n",
      "  inflating: Разметки eaf/Паленный_3_КАК.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_6_ААС.eaf  \n",
      "  inflating: Разметки eaf/Соколов_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Соловейчик_4_ААС.eaf  \n",
      "  inflating: Разметки eaf/Три_медведя_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Румянцева_ЕСЯ.eaf  \n",
      "  inflating: Разметки eaf/Цветков Сергей_СГБ.eaf  \n",
      "  inflating: Разметки eaf/Паленный_1_КАК.eaf  \n",
      "  inflating: Разметки eaf/зима_на_юге_ВПР.eaf  \n"
     ]
    }
   ],
   "source": [
    "!unzip Разметки_eaf-20211027.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "lirr295257iudgp5fldf0s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyeaf.pyeaf.EAFReader at 0x7fa43f0e82d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "er = EAFReader(directory=\"Разметки eaf\")\n",
    "er.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "tp26inkh1lvpfzfqgnzp"
   },
   "source": [
    "## Stem\n",
    "Читаем предложения и делаем им стемминг и грамматический анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "3vyc9zepms831mv2g2gvbj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sentences_rus = er.get_sentences(er.RUS)\n",
    "sentences_rsl = er.get_sentences(er.RSL_R)\n",
    "\n",
    "# sentences_rsl_left = er.get_sentences(er.RSL_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "zaz9wii4u1x4bljj1idbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Видно меня?',\n",
       " 'Там было ограничение по сумме, и ВОГ начал перевозить товары в Россию',\n",
       " 'Сам ВОГ не мог перевозить, поэтому этим управляли крутые слышащие, которым платили проценты',\n",
       " 'Начали крышевать ВОГ',\n",
       " 'Абрамов приехал в Москву с просьбой']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "sentences_rus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "nvud3ggkiw8k9h09cyughf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymystem3 in /home/jupyter/.local/lib/python3.7/site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.7/site-packages (from pymystem3) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.7/site-packages (from requests->pymystem3) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.7/site-packages (from requests->pymystem3) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.7/site-packages (from requests->pymystem3) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyter/.local/lib/python3.7/site-packages (from requests->pymystem3) (1.25.11)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "#%pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "we152mm96gjv2ufzu9cvxr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "st = TextStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "crtyq8ihe7v3y3frtgci"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "stem_sentences_rus, gram_sentences_rus = st.stem(sentences_rus, gram=True)\n",
    "stem_sentences_rsl = list(map(RSLStemmer.stem_sentence, sentences_rsl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6ug4tgm74pmgtfs6z35y5j"
   },
   "source": [
    "Так происходит потому что бывают левши, а еще бывают короткие предложения которые показывают одной ркуой левой.\n",
    "\n",
    "Типа \"следующий слайд\" можно левой рукой показать махнуть, даже если ты правша\n",
    "\n",
    "Это надо потом учесть иначе около 240 предложений выпадут из генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "7pofck0oyztjqxsi8tslz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Так происходит потому что бывают левши, а еще бывают короткие предложения которые показывают одной ркуой левой.\n",
    "# Типа \"следующий слайд\" можно левой рукой показать махнуть, даже если ты правша\n",
    "print(stem_sentences_rsl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "vsxr48gfhge0urgt3p26bp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ADVPRO'], ['сред', 'несов', 'нп', 'изъяв', 'прош', 'ед', 'V'], ['неод', 'сред', 'S', 'им', 'вин', 'ед'], ['PR'], ['неод', 'S', 'дат', 'пр', 'ед', 'жен'], ['CONJ'], ['неод', 'S', 'мн', 'род', 'жен'], ['сов', 'изъяв', 'прош', 'пе', 'ед', 'муж', 'V'], ['несов', 'инф', 'V', 'пе'], ['неод', 'S', 'мн', 'им', 'вин', 'муж'], ['PR'], ['неод', 'S', 'вин', 'ед', 'гео', 'жен']]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(gram_sentences_rus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "wycpl8oim1bgpuvjp7mmjo"
   },
   "source": [
    "## Замена существительных\\глаголов\n",
    "\n",
    "Надо как-то замену сделать тоже рандомной и прореженной, чтобы не было такого, что одно и то же предложение десять тысяч раз встречается или одни и те же замены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "ot13bjad5cnfxwvd7m80l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6096"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "len(sentences_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "3nbjqwjb06nqb6m4umpd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1219.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# вот столько я отложу для более чистого тестирования\n",
    "0.2 * 6096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "16a3x0ln8sg4pxao89rktv"
   },
   "source": [
    "Можно сделать по от 10 до 30 копий на каждое предложение и это будет прямо большой датасет\n",
    "\n",
    "Разброс от 10 до 30 нужен, чтобы рандомность какая-то там была. Потому что 6096 это мало предложений, мы не можем говорить, что там сбалансированная выборка по синтаксическим типам предложений, поэтому лучше немножко рандомизировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "h8sqwjcerpfuvy3lp88h7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "5800 * 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7qbktg5fm5qj8eqo94f9id"
   },
   "source": [
    "- number of sentences to generate from this one k = random.randint(10, 30)\n",
    "- какие позици в исходном предложении можно заменять: сущ в им.п, вин.п.  (падеж меняю прямо во время генерации, когда уже выбрала одно слово), одуш\\неодуш.; паттерны из существительного и полного правого генитива; глаголы переход\\непереход.\n",
    "- Сделать k итераций, на каждой замена всех позиций random.choice(список слов этого типа из словаря(?))\n",
    "- Сохранить список всех параллельний предложения в tsv вне цикла\n",
    "\n",
    "## Загрузим списки слов по их свойствам\n",
    "\n",
    "Из этих списков потом будем выбирать с random.choice\n",
    "\n",
    "### Сделаем списки существительных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "cellId": "clxz8qncii8bwinwgo7a76"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "with open(\"animate.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    anim_nouns = [line.strip('\\n') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "cellId": "ev46nalbfbmgooulkomwxf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "with open(\"inanimate.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inanim_nouns = [line.strip('\\n') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3gptfcviyiwzfqu4xbbp1"
   },
   "source": [
    "Разделить по роду эти два списка. Должно выйти 6 списков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "cellId": "saqi5g6telqganaap39c5u"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def split_by_gender(nouns):\n",
    "    nouns_m, nouns_f, nouns_n = [], [], []\n",
    "    for noun in nouns:\n",
    "        if 'муж' in st.stem(noun)[1][0][0]:\n",
    "            nouns_m.append(noun)\n",
    "        elif 'жен' in st.stem(noun)[1][0][0]:\n",
    "            nouns_f.append(noun)\n",
    "        elif 'сред' in st.stem(noun)[1][0][0]:\n",
    "            nouns_n.append(noun)\n",
    "    return nouns_m, nouns_f, nouns_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "cellId": "221sssleu6krc7be82xv"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "inan_m_nouns, inan_f_nouns, inan_n_nouns = split_by_gender(inanim_nouns)\n",
    "anim_m_nouns, anim_f_nouns, anim_n_nouns = split_by_gender(anim_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "cellId": "9eex856s6822v38j7bvb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "резкая боль\n",
      "гигиеническая прокладка\n",
      "горизонтальная плоскость\n",
      "губная помада\n",
      "зубная щётка\n",
      "игральная кость\n",
      "контурная линия\n",
      "личная мотивация\n",
      "мультимедийная презентация\n",
      "напольная плитка\n",
      "окружающая среда\n",
      "подводная лодка\n",
      "подъемная платформа\n",
      "спортивная ходьба\n",
      "устная презентация\n",
      "чешская республика\n",
      "шахматная игра\n",
      "эректильная дисфункция\n",
      "Слава богу\n",
      "бутсы футбольные\n",
      "емкость конденсатора\n",
      "земля почва\n",
      "касательная линия\n",
      "коробка передач\n",
      "крышка контейнера\n",
      "лампа индикатор\n",
      "линия шеи\n",
      "ось вращения\n",
      "парная игра\n",
      "пломба клеймо\n",
      "подрезка деревьев\n",
      "проверка контроль\n",
      "рубка деревьев\n",
      "ручка дверная\n",
      "серия визитов\n",
      "текстура поверхности\n",
      "угроза здоровью\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# check len\n",
    "for el in inan_f_nouns:\n",
    "    if ' ' in el:\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "cellId": "w6szkv1fgl2w27wpane5l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['дочь',\n",
       " 'племянница',\n",
       " 'бабушка',\n",
       " 'внучка',\n",
       " 'девочка',\n",
       " 'акула',\n",
       " 'бабочка',\n",
       " 'ведьма',\n",
       " 'вошь',\n",
       " 'гусеница']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "anim_f_nouns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ma8ohs074cx7wm7y8pz8s"
   },
   "source": [
    "Сюда же я загружу списки с этого гитхаба https://github.com/Koziev/NLP_Datasets \n",
    "\n",
    "* Архив patterns.noun_gen.zip содержит паттерны из двух существительных, из которых второе в родительном падеже:\n",
    "\n",
    "```\n",
    "4\tфранцузские\t<<<null>>>\n",
    "4\tдворец\tфестивалей\n",
    "4\tназванье\tмест\n",
    "4\tклассы\tвагонов\n",
    "4\tдоступность\tмагазина\n",
    "```\n",
    "\n",
    "Обратите внимание, что если в исходном предложении у генитива были подчиненные прилагательные или PP, то они в этом датасете будут удалены. Токен `<<>>` в столбце генитива обозначает ситуацию, когда первое существительное употреблено без генитива. Эти записи упрощают маргинализацию частот.\n",
    "\n",
    "* Архив patterns.noun_np_gen.zip содержит паттерны из существительного и полного правого генитива:\n",
    "\n",
    "```\n",
    "окно браузера\n",
    "течение дня\n",
    "укус медведки\n",
    "изюминка такой процедуры\n",
    "суть декларации\n",
    "рецепт вкусного молочного коктейля\n",
    "музыка самого высокого уровня\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "cellId": "jgv4g47f1t7hhgbelt1f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from string import punctuation\n",
    "\n",
    "noun_np_gen = []\n",
    "for line in open(\"patterns.noun_np_gen.txt\", \"r\", encoding=\"utf-8\"):\n",
    "    if not (set(line) & set(punctuation)):\n",
    "        noun_np_gen.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellId": "w92240scnfbocntyb50cl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949109\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(len(noun_np_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "o4zvqc96pvmuwm7pawfimi"
   },
   "source": [
    "Разделить на одушевленные\\неодушевленные по первому слову:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "cellId": "2or6sdvj5p3emlosagz7gf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['номер телефона',\n",
       " 'модель кофты',\n",
       " 'кожа лица кофе',\n",
       " 'лицо кофе',\n",
       " 'ребята гор',\n",
       " 'вид гонок',\n",
       " 'автор ответа',\n",
       " 'продажа техники',\n",
       " 'конец кода',\n",
       " 'комплекс неполноценности']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "noun_np_gen[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "cellId": "puahqx80lwrdyv3pic4bj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['висячий', 'замок']],\n",
       " [[['A', 'неод', 'ед', 'полн', 'им', 'муж', 'вин'],\n",
       "   ['неод', 'ед', 'им', 'муж', 'вин', 'S']]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:848: UserWarning: The following variables cannot be serialized: st\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "st.stem('висячий замок')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "cellId": "0jeei41h4bhvpb8r4ircutl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# разделить на одуш\\неодуш\n",
    "anim_noun_np_gen = []\n",
    "inanim_noun_np_gen = []\n",
    "for np in noun_np_gen:\n",
    "    main_word = np.split()[0]\n",
    "    if 'неод' in st.stem(main_word)[1][0][0]:\n",
    "        inanim_noun_np_gen.append(np)\n",
    "    else:\n",
    "        anim_noun_np_gen.append(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cellId": "eoa6cqgzkib950ru38z6i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ребята гор',\n",
       " 'автор ответа',\n",
       " 'поставщик товаров',\n",
       " 'отчим моего мужа',\n",
       " 'предводитель вражеской армии',\n",
       " 'детеныш дикой крысы',\n",
       " 'заведующая детсада',\n",
       " 'старейшины ресурса',\n",
       " 'селфи один глаз',\n",
       " 'королева проекта']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "anim_noun_np_gen[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "uaxf3a5lq3bp7omib7poqs"
   },
   "source": [
    "Разделить по роду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "cellId": "zwj5pn6nbpnk8x49un7q9p"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#st = TextStemmer()\n",
    "\n",
    "inanim_m_noun_np_gen, inanim_f_noun_np_gen, inanim_n_noun_np_gen = [], [], []\n",
    "for np in inanim_noun_np_gen:\n",
    "    noun = np.split()[0]\n",
    "    if 'муж' in st.stem(noun)[1][0][0]:\n",
    "        inanim_m_noun_np_gen.append(np)\n",
    "    elif 'жен' in st.stem(noun)[1][0][0]:\n",
    "        inanim_f_noun_np_gen.append(np)\n",
    "    elif 'сред' in st.stem(noun)[1][0][0]:\n",
    "        inanim_n_noun_np_gen.append(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "cellId": "zv0a00nz8djutxkmfxh1gs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['номер телефона',\n",
       " 'вид гонок',\n",
       " 'конец кода',\n",
       " 'комплекс неполноценности',\n",
       " 'массаж тела',\n",
       " 'укус медведки',\n",
       " 'рецепт вкусного молочного коктейля',\n",
       " 'спутник жизни',\n",
       " 'голос певца',\n",
       " 'скриншот целой страницы']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "inanim_m_noun_np_gen[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zecnjnc0onahdnxjr8nru"
   },
   "source": [
    "Тоже самое для одушевленных сочетаний с генитивом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "cellId": "fc2prr63tllhgrnwr9d8mr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "anim_m_noun_np_gen, anim_f_noun_np_gen, anim_n_noun_np_gen = [], [], []\n",
    "for np in anim_noun_np_gen:\n",
    "    noun = np.split()[0]\n",
    "    if 'муж' in st.stem(noun)[1][0][0]:\n",
    "        anim_m_noun_np_gen.append(np)\n",
    "    elif 'жен' in st.stem(noun)[1][0][0]:\n",
    "        anim_f_noun_np_gen.append(np)\n",
    "    elif 'сред' in st.stem(noun)[1][0][0]:\n",
    "        anim_n_noun_np_gen.append(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "cellId": "84qnbbkng9qyrp5g1jbmrh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['заведующая детсада',\n",
       " 'королева проекта',\n",
       " 'прима скорпионов для бизнеса',\n",
       " 'уполномочена полиции',\n",
       " 'дочь отношения',\n",
       " 'сестра мужа для мамы жены',\n",
       " 'мама жены',\n",
       " 'девушки полуночниц',\n",
       " 'хранительница очага',\n",
       " 'укротительница тигров']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "anim_f_noun_np_gen[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5mxgo8qaivsvnl7epe4rd8"
   },
   "source": [
    "### Сделаем списки глаголов\n",
    "\n",
    "Переходные и непереходные. Делаю из жестов, которые есть в нашем словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "cellId": "bz9yirc37cd5vp1rj6nxps"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "words = pd.read_csv(\"animation_pos.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "cellId": "rbjfelbkb1ec0fk09s8fxh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'text': 'торопиться', 'analysis': [{'wt': 1, 'lex': 'торопиться', 'gr': 'V,несов,нп=инф'}]}]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "words[words['pos'] == 'V']['analysis'][298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "cellId": "9b4skxwpcq94soy5qo18i9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Сколько тут вообще всего глаголов?\n",
    "len(words[words['pos'] == 'V']['analysis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "iu9lefh1vmd4y8u7j6nad"
   },
   "source": [
    "Делаю список непереходных глаголов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "cellId": "tsigtolvb7mmnhc3r03k1l"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "tr_verbs, intr_verbs = [], []\n",
    "for verb in words[words['pos'] == 'V'].itertuples():\n",
    "    analysis = json.loads(verb.analysis.replace('\\'', '\\\"'))\n",
    "    gr = analysis[0]['analysis'][0]['gr']\n",
    "    if 'пе' in gr:\n",
    "        tr_verbs.append(verb.text)\n",
    "    elif 'нп' in gr:\n",
    "        intr_verbs.append(verb.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "cellId": "wqc09ms4ub8wzuvkd2k6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'приостановиться'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "random.choice(intr_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "cellId": "hp0hmq08ywrho60v1zp6ip"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "L = [[[3]], [3, 4]]\n",
    "depth = lambda L: isinstance(L, list) and max(map(depth, L))+1\n",
    "depth(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellId": "l4ykcd53dyng8mlr6zylm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3], [3, 4]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "list(drop_inner_nest(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5tyf9oov3m8zy89jmzr2p"
   },
   "source": [
    "### Генерируем предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "cellId": "3e7th6a4kfxnxink5dn62"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "depth = lambda L: isinstance(L, list) and max(map(depth, L))+1\n",
    "\n",
    "def flatten(S):\n",
    "    if S == []:\n",
    "        return S\n",
    "    if isinstance(S[0], list):\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    return S[:1] + flatten(S[1:])\n",
    "\n",
    "def drop_inner_nest(S):\n",
    "    for sublist in S:\n",
    "        if isinstance(sublist[0], list):\n",
    "            yield sublist[0]\n",
    "        else:\n",
    "            yield sublist\n",
    "\n",
    "def create_gr_n_tags(gr, old_gram_sent_rus):\n",
    "    # keep original case and number\n",
    "    tags = ['им', 'вин', 'род', 'дат', 'твор', 'пр', 'парт', 'местн', 'зват', 'ед', 'мн']\n",
    "    initial_tags = [el for el in old_gram_sent_rus if el in tags]\n",
    "    for i, word_gr in enumerate(gr):\n",
    "        gr[i] = [el for el in word_gr if el not in tags]\n",
    "        gr[i].extend(initial_tags)\n",
    "    return gr\n",
    "\n",
    "def create_rsl_gloss(old_rus_lemma, new_rus_lemma, old_stem_sent_rus: list, old_stem_rsl: list):\n",
    "    \"\"\"creates rsl glosses for the whole stemmed rsl sentence\"\"\"\n",
    "    # update stemmed rsl sentence\n",
    "    # return full rsl sentence as a list, not just updated words\n",
    "    # this does not work well because of things like посмотреть\\смотреть\n",
    "    new_stem_rsl = copy.deepcopy(old_stem_rsl)\n",
    "    #print('create_rsl_gloss')\n",
    "    #print('new_rus_lemma', new_rus_lemma)\n",
    "    found = False\n",
    "    for i, gloss in enumerate(old_stem_rsl):\n",
    "        if gloss == old_rus_lemma:\n",
    "            #print('rus lemma based change')\n",
    "            new_stem_rsl[i] = new_rus_lemma\n",
    "            found = True\n",
    "    if not found:\n",
    "        # so we have to use index replacement as well\n",
    "        for j, gloss in enumerate(old_stem_sent_rus):\n",
    "            if gloss == old_rus_lemma and j < len(new_stem_rsl):\n",
    "                #print('index based change')\n",
    "                new_stem_rsl[j] = new_rus_lemma\n",
    "    #print(\"new_stem_rsl on this step\")\n",
    "    #pprint(new_stem_rsl)\n",
    "    return new_stem_rsl\n",
    "\n",
    "def make_replacments(replacement, old_stem_sent_rus_one, old_gram_sent_rus_one, old_stem_sent_rus: list, old_stem_rsl: list, keep_gr=2):\n",
    "    old_rus_lemma = old_stem_sent_rus_one\n",
    "    old_stem_sent_rus_one, gr = st.stem(replacement)\n",
    "    new_rus_lemma = old_stem_sent_rus_one[0]\n",
    "    if keep_gr==1:\n",
    "        old_gram_sent_rus_one = create_gr_n_tags(gr, old_gram_sent_rus_one)\n",
    "    elif keep_gr==0:\n",
    "        old_gram_sent_rus_one = gr\n",
    "    new_stem_rsl_one = create_rsl_gloss(old_rus_lemma, new_rus_lemma, old_stem_sent_rus, old_stem_rsl)\n",
    "    return old_stem_sent_rus_one, old_gram_sent_rus_one, new_stem_rsl_one\n",
    "\n",
    "def pick_and_replace(old_stem_sent_rus_one, old_gram_sent_rus_one, old_stem_sent_rus, old_stem_rsl, choices, complex_choices=None):\n",
    "    if complex_choices:\n",
    "        # decide between single noun of np with gen\n",
    "        pick = random.randint(0,1)  # 0 is single noun, 1 is np with gen\n",
    "        if not bool(pick):\n",
    "            choice = random.choice(choices)\n",
    "            if len(choice.split()) > 1:\n",
    "                old_stem_sent_rus_one, old_gram_sent_rus_one, new_stem_rsl = make_replacments(choice, old_stem_sent_rus_one, old_gram_sent_rus_one, old_stem_sent_rus, old_stem_rsl, keep_gr=1)\n",
    "                # this line is in the wrong place\n",
    "            else:\n",
    "                old_stem_sent_rus_one, old_gram_sent_rus_one, new_stem_rsl = make_replacments(choice, old_stem_sent_rus_one, old_gram_sent_rus_one, old_stem_sent_rus, old_stem_rsl, keep_gr=2)\n",
    "        else:\n",
    "            choice = random.choice(complex_choices)\n",
    "            old_stem_sent_rus_one, old_gram_sent_rus_one, new_stem_rsl = make_replacments(choice, old_stem_sent_rus_one, old_gram_sent_rus_one, old_stem_sent_rus, old_stem_rsl, keep_gr=0)\n",
    "    else: \n",
    "        choice = random.choice(choices)\n",
    "        old_stem_sent_rus_one, old_gram_sent_rus_one, new_stem_rsl = make_replacments(choice, old_stem_sent_rus_one, old_gram_sent_rus_one, old_stem_sent_rus, old_stem_rsl, keep_gr=0)\n",
    "    return old_stem_sent_rus_one, old_gram_sent_rus_one, new_stem_rsl\n",
    "\n",
    "def generate_parallel_sentence(old_gram_sent_rus_init, old_stem_sent_rus_init, old_stem_rsl_init):\n",
    "    \"\"\"this function generates one parallel sentence\"\"\"\n",
    "    old_gram_sent_rus = copy.deepcopy(old_gram_sent_rus_init)\n",
    "    old_stem_sent_rus = copy.deepcopy(old_stem_sent_rus_init)\n",
    "    old_stem_rsl = copy.deepcopy(old_stem_rsl_init)\n",
    "    number_replacements = 0   \n",
    "    for k, gram in enumerate(old_gram_sent_rus_init):\n",
    "        # gender is relevant for nouns because of concord \n",
    "        if 'S' and 'од' and 'муж' in gram:\n",
    "            number_replacements += 1\n",
    "            old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_rsl = pick_and_replace(old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_sent_rus, old_stem_rsl, anim_m_nouns, complex_choices=anim_m_noun_np_gen)\n",
    "        elif 'S' and 'неод'  and 'муж' in gram:\n",
    "            number_replacements += 1 \n",
    "            old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_rsl = pick_and_replace(old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_sent_rus, old_stem_rsl, inan_m_nouns, complex_choices=inanim_m_noun_np_gen)\n",
    "        elif 'S' and 'од' and 'жен' in gram:\n",
    "            number_replacements += 1\n",
    "            old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_rsl = pick_and_replace(old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_sent_rus, old_stem_rsl, anim_f_nouns, complex_choices=anim_f_noun_np_gen)\n",
    "        elif 'S' and 'неод'  and 'жен' in gram:\n",
    "            number_replacements += 1\n",
    "            old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_rsl = pick_and_replace(old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_sent_rus, old_stem_rsl, inan_f_nouns, complex_choices=inanim_f_noun_np_gen)\n",
    "        elif 'S' and 'од' and 'сред' in gram:\n",
    "            number_replacements += 1\n",
    "            old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_rsl = pick_and_replace(old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_sent_rus, old_stem_rsl, anim_n_nouns, complex_choices=anim_n_noun_np_gen)\n",
    "        elif 'S' and 'неод'  and 'сред' in gram:\n",
    "            number_replacements += 1\n",
    "            old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_rsl = pick_and_replace(old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_sent_rus, old_stem_rsl, inan_n_nouns, complex_choices=inanim_n_noun_np_gen)\n",
    "        elif 'V' and 'пе' in gram:\n",
    "            number_replacements += 1\n",
    "            old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_rsl = pick_and_replace(old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_sent_rus, old_stem_rsl, tr_verbs)\n",
    "        elif 'V' and 'нп' in gram:\n",
    "            number_replacements += 1\n",
    "            old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_rsl = pick_and_replace(old_stem_sent_rus[k], old_gram_sent_rus[k], old_stem_sent_rus, old_stem_rsl, intr_verbs)\n",
    "\n",
    "        # если делать больше 5 замен, получается полный бред\n",
    "        # надо еще сделать замены не подряд, а типа рандомно пропускать шаги??\n",
    "        # может после переходного глагола всегда пропускать,\n",
    "        # потому что очень сложно случайным образом подобрать правильный прямой объект или и того хуже инфинитивный оборот\n",
    "        if number_replacements == 5:\n",
    "            break\n",
    "            \n",
    "    if number_replacements == 0:\n",
    "        # havent found anything to replace\n",
    "        return None, None, None\n",
    "    \n",
    "    # randomly pick word\n",
    "    # insert constituent in old_stem_sent_rus (stem\\gram before insertion)\n",
    "    # insert gram of constituent in old_gram_sent_rus it should inherent everything from the original word\n",
    "    # but if the length of replacement > 1, \n",
    "    # insert stemmed constituent in old_stem_rsl\n",
    "    # flatten lists of depth 3\n",
    "    if depth(old_gram_sent_rus) > 2:\n",
    "        old_gram_sent_rus = list(drop_inner_nest(list(drop_inner_nest(old_gram_sent_rus))))\n",
    "\n",
    "    return old_gram_sent_rus, list(flatten(old_stem_sent_rus)), list(flatten(old_stem_rsl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellId": "mpx6q8rspyp7x3s33dcec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\n",
      "['там', 'быть', 'ограничение', 'по', 'сумма', 'и', 'вога', 'начинать', 'перевозить', 'товар', 'в', 'россия'] []\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(len(stem_sentences_rus[0]), len(stem_sentences_rsl[0]))\n",
    "print(stem_sentences_rus[1], stem_sentences_rsl[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "czpub5rh9ji72qnws13vhj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['человек',\n",
       "  'и',\n",
       "  'животное',\n",
       "  'получать',\n",
       "  'хороший',\n",
       "  'физический',\n",
       "  'нагрузка',\n",
       "  'и',\n",
       "  'много',\n",
       "  'положительный',\n",
       "  'эмоция'],\n",
       " ['пожарный',\n",
       "  'решать',\n",
       "  'навещать',\n",
       "  'собака',\n",
       "  'и',\n",
       "  'свой',\n",
       "  'активность',\n",
       "  'они',\n",
       "  'показывать',\n",
       "  'прохожий',\n",
       "  'что',\n",
       "  'собака',\n",
       "  'нуждаться',\n",
       "  'в',\n",
       "  'любовь',\n",
       "  'и',\n",
       "  'дом']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "stem_sentences_rus[240:242]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "tyvyf6otkwk4s7bygk0f6a"
   },
   "source": [
    "### Отделение тестовой выборки\n",
    "\n",
    "Чтобы быть уверенными, что это прямо реально zero-shot. И есть предложения на основе, которых мы не генерировали обучающую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "cellId": "51l96rygbu6qbbj5429psl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stem_sents_rus, test_stem_rus, gram_sents_rus, test_gram_rus, stem_sents_rsl, test_rsl = train_test_split(stem_sentences_rus,\n",
    "                                                                                                          gram_sentences_rus,\n",
    "                                                                                                          stem_sentences_rsl,\n",
    "                                                                                                          test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "cellId": "xoo0ztmp4k4xqy0v5e2a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4876/4876 [00:50<00:00, 97.41it/s] \n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# test test test\n",
    "from tqdm import tqdm\n",
    "\n",
    "new_gram_sents_rus, new_stem_sents_rus, new_stem_rsls = [], [], []\n",
    "\n",
    "for n, gram in enumerate(tqdm(gram_sents_rus)):\n",
    "    k = random.randint(10, 30)\n",
    "    for i in range(k): \n",
    "        #generate one parallel sentence\n",
    "        \n",
    "        if stem_sents_rsl[n] is None or stem_sents_rsl[n] == []:\n",
    "            break\n",
    "            \n",
    "        #print(\"INITIAL GRAM\", gram_sents_rus[n])\n",
    "        #print(\"INITIAL RUS\", stem_sents_rus[n])\n",
    "        #print(\"INITIAL RSL\", stem_sents_rsl[n])\n",
    "        new_gram_sent_rus, new_stem_sent_rus, new_stem_rsl = generate_parallel_sentence(gram_sents_rus[n],\n",
    "                                                                                        stem_sents_rus[n],\n",
    "                                                                                        stem_sents_rsl[n])\n",
    "        \n",
    "        new_gram_sents_rus.append(new_gram_sent_rus)\n",
    "        new_stem_sents_rus.append(new_stem_sent_rus)\n",
    "        new_stem_rsls.append(new_stem_rsl)\n",
    "        #print(\"NEW GRAM \", new_gram_sent_rus)\n",
    "        #print(\"NEW RUS \", new_stem_sent_rus)\n",
    "        #print(\"NEW RSL \", new_stem_rsl)\n",
    "        #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "cellId": "4ebf8xwx7oy26826yck2dxh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# save to some tsv file\n",
    "generated_dict = {'new_stem_sent_rus': new_stem_sents_rus, 'new_gram_sent_rus': new_gram_sents_rus, 'new_stem_rsl': new_stem_rsls}\n",
    "generated_data = pd.DataFrame.from_dict(generated_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "cellId": "f9dnn5lxreag6aoc098b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_stem_sent_rus</th>\n",
       "      <th>new_gram_sent_rus</th>\n",
       "      <th>new_stem_rsl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[пожалуйста, расставаться, мы, казак, зарубежь...</td>\n",
       "      <td>[[PART], [инф, нп, сов, V], [SPRO, 1-л, мн, да...</td>\n",
       "      <td>[1ps, просить, 2ps, расставаться, 2ps, 1ps, ес...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[пожалуйста, нападать, мы, чемпион, если, не, вы]</td>\n",
       "      <td>[[PART], [несов, сов, инф, нп, V], [SPRO, 1-л,...</td>\n",
       "      <td>[1ps, просить, 2ps, нападать, 2ps, 1ps, если, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[пожалуйста, сомневаться, мы, хозяин, ресторан...</td>\n",
       "      <td>[[PART], [инф, нп, несов, V], [SPRO, 1-л, мн, ...</td>\n",
       "      <td>[1ps, просить, 2ps, сомневаться, 2ps, 1ps, есл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[пожалуйста, оставаться, мы, краб, если, не, вы]</td>\n",
       "      <td>[[PART], [полн, им, вин, несов, неод, ед, дейс...</td>\n",
       "      <td>[1ps, просить, 2ps, оставаться, 2ps, 1ps, если...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[пожалуйста, жениться, мы, инвалид, если, не, вы]</td>\n",
       "      <td>[[PART], [несов, сов, инф, нп, V], [SPRO, 1-л,...</td>\n",
       "      <td>[1ps, просить, 2ps, жениться, 2ps, 1ps, если, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   new_stem_sent_rus  \\\n",
       "0  [пожалуйста, расставаться, мы, казак, зарубежь...   \n",
       "1  [пожалуйста, нападать, мы, чемпион, если, не, вы]   \n",
       "2  [пожалуйста, сомневаться, мы, хозяин, ресторан...   \n",
       "3   [пожалуйста, оставаться, мы, краб, если, не, вы]   \n",
       "4  [пожалуйста, жениться, мы, инвалид, если, не, вы]   \n",
       "\n",
       "                                   new_gram_sent_rus  \\\n",
       "0  [[PART], [инф, нп, сов, V], [SPRO, 1-л, мн, да...   \n",
       "1  [[PART], [несов, сов, инф, нп, V], [SPRO, 1-л,...   \n",
       "2  [[PART], [инф, нп, несов, V], [SPRO, 1-л, мн, ...   \n",
       "3  [[PART], [полн, им, вин, несов, неод, ед, дейс...   \n",
       "4  [[PART], [несов, сов, инф, нп, V], [SPRO, 1-л,...   \n",
       "\n",
       "                                        new_stem_rsl  \n",
       "0  [1ps, просить, 2ps, расставаться, 2ps, 1ps, ес...  \n",
       "1  [1ps, просить, 2ps, нападать, 2ps, 1ps, если, ...  \n",
       "2  [1ps, просить, 2ps, сомневаться, 2ps, 1ps, есл...  \n",
       "3  [1ps, просить, 2ps, оставаться, 2ps, 1ps, если...  \n",
       "4  [1ps, просить, 2ps, жениться, 2ps, 1ps, если, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "generated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "cellId": "fk6fymwnnxf5oumblxoy67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['APRO', 'неод', 'вин', 'мн', 'им'],\n",
       " ['полн', 'им', 'вин', 'ед', 'сред', 'A'],\n",
       " ['им', 'мн', 'муж', 'од', 'S'],\n",
       " ['дат', 'твор', 'ед', 'неод', 'S', 'вин', 'мн', 'муж', 'им', 'род', 'пр'],\n",
       " ['инф', 'пе', 'несов', 'V'],\n",
       " ['PR'],\n",
       " ['SPRO', 'род', 'вин']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "new_gram_sents_rus[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "cellId": "nku0lya7u5ki7qnyrukvjm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "len(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "cellId": "n7l73evfhy3602gebgp6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#compression_opts = dict(method='zip', archive_name='generated_stemed.csv') \n",
    "#generated_data.to_csv('generated_stemed.zip', index=False, compression=compression_opts)  \n",
    "generated_data.to_csv('generated_stemed.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "i7jmixn3sx5pmb5d4o3nx"
   },
   "source": [
    "## Load previously generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "cellId": "5g686so4pqgwekaso8ro"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "generated_data = pd.read_csv('generated_stemed.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "cellId": "7uhmyyyhzquz7miu7j8057"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "generated_data = generated_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "cellId": "cpvb67lud3r0p4ujtb52e2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "for row in generated_data.itertuples():\n",
    "    if isinstance(row[1], float):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "cellId": "k2wi56o2w5mb639svf111h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "generated_data['new_stem_sent_rus'] = generated_data['new_stem_sent_rus'].apply(lambda sent: sent.strip('[]\\'').split('\\', \\''))\n",
    "generated_data['new_stem_rsl'] = generated_data['new_stem_rsl'].apply(lambda sent: sent.strip('[]\\'').split('\\', \\''))\n",
    "\n",
    "#generated_data['new_stem_sent_rus'][0].strip('[]\\'').split('\\', \\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "cellId": "ocrekll98bnnhescmh4org"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "generated_data['new_gram_sent_rus'] = generated_data['new_gram_sent_rus'].apply(lambda sent: json.loads(sent.replace('\\'', '\\\"')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "u68jqr6hssj20gc22fau"
   },
   "source": [
    "# Обучаем модель (о да)\n",
    "\n",
    "### Сначала попробую не прореживать данные и взять все\n",
    "\n",
    "Можно не прогонять остальные клетки кода, а просто импортировать файл `generated_stemed.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xgpboho203jix0zneywy6f"
   },
   "source": [
    "Мне не нужно делать сплит, я могу попробовать учить на всех данных, а тестировать на исходных, типа zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "cellId": "dmug9trxcy77h7w21ls"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_rus, test_rus, train_gram_rus, test_gram_rus, train_rsl, test_rsl = train_test_split(new_stem_sents_rus,\n",
    "                                                                                           new_gram_sents_rus,\n",
    "                                                                                           new_stem_rsls,\n",
    "                                                                                           test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "cellId": "sltdd8ht46lc1vuh9a71tf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "voc_rus = VocabularyVectorizer(phrase_border=True)\n",
    "bin_gram = GramBinarizer(phrase_border=True)\n",
    "voc_rsl = VocabularyVectorizer(phrase_border=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "cellId": "74evn430d6adotb8wyk9w4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "type(train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "cellId": "sg9rf94adeu0js9wopis"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-b91aaa1ef808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvoc_rus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc_rus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbin_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbin_gram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gram_rus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvoc_rsl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc_rsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rsl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/pyeaf/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/pyeaf/text.py\u001b[0m in \u001b[0;36m_add_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/pyeaf/text.py\u001b[0m in \u001b[0;36m_add_phrase\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrase_border\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "voc_rus = voc_rus.fit(train_rus)\n",
    "bin_gram = bin_gram.fit(train_gram_rus)\n",
    "voc_rsl = voc_rsl.fit(train_rsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "7dqe6umtkz58zi296qpekv"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "vec_sentences_rus_train = voc_rus.text_to_index(train_rus)\n",
    "vec_gram_train = bin_gram.transform(train_gram_rus)\n",
    "vec_sentences_rsl_train = voc_rsl.text_to_index(train_rsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "cellId": "no5haprmhi9leuyzygoe8r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73690, 4965, 50, 3214, 62)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "len(new_stem_sents_rus), voc_rus.word_count, voc_rus.max_len, voc_rsl.word_count, voc_rsl.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "cellId": "j0oqwk9dtbop7nbuyvspfj"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-fcedcd4a6511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_stem_rsls\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-fcedcd4a6511>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_stem_rsls\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from collections import Counter\n",
    "\n",
    "words = [w for s in new_stem_rsls for w in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "cellId": "s82gqvyy98qbqntjrl98c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "Counter(words)['prtcl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "npl6656juein5qgu7halzf"
   },
   "source": [
    "## Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "cellId": "g15p8wjdk4pf87cck20b"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from model.model import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "cellId": "5io20u3d2qtcowrwrzvsj7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "    \n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"Normalize tensor sizes\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.Tensor): the output of the model\n",
    "            If a 3-dimensional tensor, reshapes to a matrix\n",
    "        y_true (torch.Tensor): the target predictions\n",
    "            If a matrix, reshapes to be a vector\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "cellId": "6iyf9xxps1nw2o99ofpvc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import random \n",
    "\n",
    "def batch_generator(rus_data, rsl_data, batch_size=32):\n",
    "    rus_data = np.array(rus_data)\n",
    "    rsl_data = np.array(rsl_data)\n",
    "    \n",
    "    data_length = len(rus_data)\n",
    "    tail_length = batch_size - data_length % batch_size\n",
    "    index = list(range(data_length))\n",
    "    random.shuffle(index)\n",
    "    \n",
    "    index = np.array(index + random.choices(index, k=tail_length))\n",
    "    num_batches = len(index) // batch_size\n",
    "    index = index.reshape((num_batches, batch_size))\n",
    "    \n",
    "    for batch_ind, inds in enumerate(tqdm(index)):\n",
    "        yield batch_ind, torch.tensor(rus_data[inds]), torch.tensor(rsl_data[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "cellId": "wdii2mcx6onbhwinnt29kp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    seed=1337,\n",
    "    learning_rate=5e-4,\n",
    "    batch_size=64,\n",
    "    num_epochs=30,  # 30\n",
    "    rus_emb_size=16,\n",
    "    rsl_emb_size = 16,\n",
    "    rnn_size = 64,\n",
    "    early_stopping_criteria=5,\n",
    "    mask_index = voc_rsl.mask_ind\n",
    ")\n",
    "\n",
    "set_seed_everywhere(args.seed, torch.cuda.is_available())\n",
    "\n",
    "model = Translator(voc_rus.word_count, args.rus_emb_size, voc_rsl.word_count, args.rsl_emb_size, args.rnn_size, voc_rsl.bos_ind)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "cellId": "a9kewepwfgwapm7anc9hgk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:30<00:00,  2.55it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.60it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.80it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.76it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.68it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.58it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.74it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.77it/s]\n",
      "100%|██████████| 77/77 [00:26<00:00,  2.85it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.58it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.70it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.74it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.78it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.57it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.69it/s]\n",
      "100%|██████████| 77/77 [00:30<00:00,  2.55it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.62it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.75it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.77it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.58it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.57it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.71it/s]\n",
      "100%|██████████| 77/77 [00:27<00:00,  2.80it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.62it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.66it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.67it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.75it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.73it/s]\n",
      "100%|██████████| 77/77 [00:29<00:00,  2.61it/s]\n",
      "100%|██████████| 77/77 [00:28<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  6.1725486903995685 \tacc:  27.586206896551722 \tsample_prob:  0.05\n",
      "Epoch:  1 Loss:  5.013691728765313 \tacc:  26.778242677824267 \tsample_prob:  0.05\n",
      "Epoch:  2 Loss:  4.8191376599398525 \tacc:  30.416666666666664 \tsample_prob:  0.05\n",
      "Epoch:  3 Loss:  4.484796570493028 \tacc:  36.98979591836735 \tsample_prob:  0.05\n",
      "Epoch:  4 Loss:  4.246925604807866 \tacc:  34.146341463414636 \tsample_prob:  0.05\n",
      "Epoch:  5 Loss:  3.952063675050611 \tacc:  40.73226544622426 \tsample_prob:  0.05\n",
      "Epoch:  6 Loss:  3.6457010863663317 \tacc:  41.60839160839161 \tsample_prob:  0.05\n",
      "Epoch:  7 Loss:  3.3322053543933023 \tacc:  47.90940766550523 \tsample_prob:  0.05\n",
      "Epoch:  8 Loss:  3.0339383212002833 \tacc:  58.536585365853654 \tsample_prob:  0.05\n",
      "Epoch:  9 Loss:  2.7445455619267047 \tacc:  58.35189309576837 \tsample_prob:  0.05\n",
      "Epoch:  10 Loss:  2.490479747970383 \tacc:  62.21294363256785 \tsample_prob:  0.05\n",
      "Epoch:  11 Loss:  2.250601181736241 \tacc:  67.83625730994152 \tsample_prob:  0.05\n",
      "Epoch:  12 Loss:  2.0226153181744855 \tacc:  68.89692585895118 \tsample_prob:  0.05\n",
      "Epoch:  13 Loss:  1.8187553418147093 \tacc:  75.49407114624506 \tsample_prob:  0.05\n",
      "Epoch:  14 Loss:  1.6323601735102664 \tacc:  77.5330396475771 \tsample_prob:  0.05\n",
      "Epoch:  15 Loss:  1.4736910191449248 \tacc:  76.87366167023555 \tsample_prob:  0.06666666666666667\n",
      "Epoch:  16 Loss:  1.3117453184994785 \tacc:  84.7345132743363 \tsample_prob:  0.13333333333333333\n",
      "Epoch:  17 Loss:  1.1776014132933188 \tacc:  85.76779026217228 \tsample_prob:  0.2\n",
      "Epoch:  18 Loss:  1.063106721097773 \tacc:  86.86131386861314 \tsample_prob:  0.26666666666666666\n",
      "Epoch:  19 Loss:  0.9623709955772795 \tacc:  87.66816143497758 \tsample_prob:  0.3333333333333333\n",
      "Epoch:  20 Loss:  0.8589485976603123 \tacc:  89.94082840236686 \tsample_prob:  0.4\n",
      "Epoch:  21 Loss:  0.7800688534588008 \tacc:  91.0377358490566 \tsample_prob:  0.4666666666666667\n",
      "Epoch:  22 Loss:  0.7017276465118705 \tacc:  93.12039312039312 \tsample_prob:  0.5333333333333333\n",
      "Epoch:  23 Loss:  0.6304750152222524 \tacc:  93.125 \tsample_prob:  0.6\n",
      "Epoch:  24 Loss:  0.5733330617477368 \tacc:  93.03675048355899 \tsample_prob:  0.6666666666666666\n",
      "Epoch:  25 Loss:  0.5218910067886502 \tacc:  95.8041958041958 \tsample_prob:  0.7333333333333333\n",
      "Epoch:  26 Loss:  0.46959158042808624 \tacc:  94.71544715447155 \tsample_prob:  0.8\n",
      "Epoch:  27 Loss:  0.42678877169435675 \tacc:  96.84684684684684 \tsample_prob:  0.8666666666666667\n",
      "Epoch:  28 Loss:  0.38658554290796254 \tacc:  97.27272727272728 \tsample_prob:  0.9333333333333333\n",
      "Epoch:  29 Loss:  0.34936708637646263 \tacc:  99.03288201160542 \tsample_prob:  1.0\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    # sample_probability = (10 + epoch_index) / args.num_epochs\n",
    "    if epoch_index < 0.5 * args.num_epochs:\n",
    "        sample_probability = 0.05\n",
    "    else:\n",
    "        sample_probability = ( 2 * (epoch_index+1) - args.num_epochs) / args.num_epochs\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_ind, rus_batch, rsl_batch in batch_generator(vec_sentences_rus_train, vec_sentences_rsl_train, args.batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(rus_batch, rsl_batch, 0.0)\n",
    "        \n",
    "        loss = sequence_loss(y_pred, rsl_batch, args.mask_index)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_ind + 1)\n",
    "        acc_t = compute_accuracy(y_pred, rsl_batch, args.mask_index)\n",
    "        \n",
    "    print('Epoch: ', epoch_index, 'Loss: ', running_loss, '\\tacc: ', acc_t, '\\tsample_prob: ', sample_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "cellId": "pv051414ionzqcg4flo1e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "cellId": "02flq4w7hhrifyiv38r8gvs"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "regex = re.compile(r'(?:<bos>|<eos>.*)')\n",
    "def pred_vs_rsl(tensor, true_rsl, rus):\n",
    "    \n",
    "    tensor = tensor.argmax(2).tolist()\n",
    "    \n",
    "    pred_rsl = voc_rsl.index_to_text(tensor)\n",
    "    true_rsl = voc_rsl.index_to_text(true_rsl)\n",
    "    rus = voc_rus.index_to_text(rus)\n",
    "                   \n",
    "    f = lambda x: regex.sub('', \" \".join(x))\n",
    "                   \n",
    "    pred_rsl = [f(sentence) for sentence in pred_rsl]\n",
    "    true_rsl = [f(sentence) for sentence in true_rsl]\n",
    "    true_rus = [f(sentence) for sentence in rus]\n",
    "    \n",
    "    return pred_rsl, true_rsl, true_rus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qfuon8f9uyibqbua5jdfg"
   },
   "source": [
    "### Тестируем модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "cellId": "ud7881s6bv6lsqjzh0b5g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "vec_sentences_rus_test = voc_rus.text_to_index(test_rus)\n",
    "vec_gram_test = bin_gram.transform(test_gram_rus)\n",
    "vec_sentences_rsl_test = voc_rsl.text_to_index(test_rsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "cellId": "v5udk5cll2fenm8t0zdeqr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "y_pred = model(torch.tensor(vec_sentences_rus_test), torch.tensor(vec_sentences_rsl_test), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "cellId": "rbrn3eyhhwp1kp9fhrufje"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "trans, truth_rsl, rus = pred_vs_rsl(y_pred,\n",
    "                                    torch.tensor(vec_sentences_rsl_test),\n",
    "                                    torch.tensor(vec_sentences_rus_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "cellId": "85c9sj14a5os17wuvnn0v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSL:   но быть руководитель простой не.хотеть принимать \n",
      "TRANS:   но быть руководитель простой не.хотеть принимать \n",
      "RUS:   но бывать руководитель просто не хотеть принимать \n",
      "\n",
      "\n",
      "RSL:   \n",
      "TRANS:   \n",
      "RUS:   в общий я много где выступать в разный город \n",
      "\n",
      "\n",
      "RSL:   \n",
      "TRANS:   \n",
      "RUS:   у я нет машина \n",
      "\n",
      "\n",
      "RSL:   обращение 2ps крепкий <unk> и прощаться \n",
      "TRANS:   старый 2ps крепкий радостный и прощаться \n",
      "RUS:   <unk> крепко и <unk> \n",
      "\n",
      "\n",
      "RSL:   следующий2 мировоззрение восприятие думать мнение pl о мир2 у.всех.одинаково \n",
      "TRANS:   следующий2 вспоминать можно думать мнение pl о мир2 у.всех.одинаково \n",
      "RUS:   следующий одинаковый мировоззрение и восприятие мир \n",
      "\n",
      "\n",
      "RSL:   1ps видеть не мочь человек молодой н-о 3ps <unk> 1ps о clf два.человека <dact> мочь \n",
      "TRANS:   1ps видеть не мочь человек молодой н-о 3ps необходимость 1ps о clf к <dact> мочь \n",
      "RUS:   я не мочь принимать это потому что он быть молодой но он заботиться о мы с дочка как мочь \n",
      "\n",
      "\n",
      "RSL:   \n",
      "TRANS:   \n",
      "RUS:   я идти гриб собирать и смотреть парень военный с ружье лежать \n",
      "\n",
      "\n",
      "RSL:   сейчас 1ps рассказать 2ps время год \n",
      "TRANS:   сейчас 1ps рассказать 2ps время год \n",
      "RUS:   сейчас я рассказывать вы о время год \n",
      "\n",
      "\n",
      "RSL:   \n",
      "TRANS:   \n",
      "RUS:   однако учитель код у мы мало \n",
      "\n",
      "\n",
      "RSL:   что включать помогать что пример \n",
      "TRANS:   что включать помогать что пример \n",
      "RUS:   в чем <unk> этот помощь \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# lets have a look\n",
    "for i in range(10):\n",
    "    print(\"RSL: \", truth_rsl[i])\n",
    "    print(\"TRANS: \", trans[i])\n",
    "    print(\"RUS: \", rus[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "gp3kn88yxhng1aov9qds0c"
   },
   "source": [
    "### Теперь считаем bleu-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "cellId": "gaunndj8zquktoxrtdg49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1220, 1220)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "len(trans), len(truth_rsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "cellId": "k7aef6rqm1t69e8vuiq7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Russian to RSL:  72.08847139943303\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import sacrebleu\n",
    "\n",
    "rus_rsl_bleu = sacrebleu.corpus_bleu(trans, [truth_rsl])\n",
    "print(\"--------------------------\")\n",
    "print(\"Russian to RSL: \", rus_rsl_bleu.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hefrne6djk8ji9xvh1lv4e"
   },
   "source": [
    "# Теперь zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "cellId": "t5plwx54lwl4yl6129emfn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "voc_rus = VocabularyVectorizer(phrase_border=True)\n",
    "bin_gram = GramBinarizer(phrase_border=True)\n",
    "voc_rsl = VocabularyVectorizer(phrase_border=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "cellId": "6ptnjq9ingorhqmqfsm6hn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "voc_rus = voc_rus.fit(list(generated_data['new_stem_sent_rus']))  # list(generated_data['new_stem_sent_rus']\n",
    "bin_gram = bin_gram.fit(list(generated_data['new_gram_sent_rus']))\n",
    "voc_rsl = voc_rsl.fit(list(generated_data['new_stem_rsl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "cellId": "a3iyqn4ipj71cypv4g8vab"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "vec_sentences_rus_train = voc_rus.text_to_index(list(generated_data['new_stem_sent_rus']))\n",
    "vec_gram_train = bin_gram.transform(list(generated_data['new_gram_sent_rus']))\n",
    "vec_sentences_rsl_train = voc_rsl.text_to_index(list(generated_data['new_stem_rsl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "cellId": "z4ytzbirzrh95dlvx358l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59154, 57604, 14763, 61, 14707, 62)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "len(new_stem_sents_rus), len(generated_data['new_stem_sent_rus']), voc_rus.word_count, voc_rus.max_len, voc_rsl.word_count, voc_rsl.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "cellId": "kepqq3pasnj2r6plgxhl6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "words = [w for s in list(generated_data['new_stem_rsl']) for w in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3x2strr0xzvgo5ex9ayuds"
   },
   "source": [
    "### Обучаем модель на всех-всех сгенерированных данных\n",
    "\n",
    "плюс тут удалены пустые строки ржя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "cellId": "oi4uxac3zptup885ekhel"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    seed = 1337,\n",
    "    learning_rate = 5e-4, \n",
    "    batch_size = 96,  # 64\n",
    "    num_epochs = 10,  # 30\n",
    "    rus_emb_size = 16,  # 16\n",
    "    rsl_emb_size = 16,  # 16\n",
    "    rnn_size = 64,  # 64\n",
    "    early_stopping_criteria = 5,\n",
    "    mask_index = voc_rsl.mask_ind\n",
    ")\n",
    "\n",
    "set_seed_everywhere(args.seed, torch.cuda.is_available())\n",
    "\n",
    "model_zero = Translator(voc_rus.word_count, args.rus_emb_size, voc_rsl.word_count, args.rsl_emb_size, args.rnn_size, voc_rsl.bos_ind)\n",
    "\n",
    "optimizer = optim.Adam(model_zero.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "cellId": "2b8cz5weqsnxpbntku529"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [14:58<00:00,  1.49s/it]\n",
      "100%|██████████| 601/601 [14:42<00:00,  1.47s/it]\n",
      "100%|██████████| 601/601 [14:41<00:00,  1.47s/it]\n",
      "100%|██████████| 601/601 [14:41<00:00,  1.47s/it]\n",
      "100%|██████████| 601/601 [14:34<00:00,  1.46s/it]\n",
      "100%|██████████| 601/601 [14:44<00:00,  1.47s/it]\n",
      "100%|██████████| 601/601 [14:30<00:00,  1.45s/it]\n",
      "100%|██████████| 601/601 [14:42<00:00,  1.47s/it]\n",
      "100%|██████████| 601/601 [14:30<00:00,  1.45s/it]\n",
      "100%|██████████| 601/601 [14:27<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  6.045795564445204 \tacc:  32.06349206349206 \tsample_prob:  0.05\n",
      "Epoch:  1 Loss:  3.7826472832240166 \tacc:  61.92893401015228 \tsample_prob:  0.05\n",
      "Epoch:  2 Loss:  1.936426304937799 \tacc:  84.00918133129304 \tsample_prob:  0.05\n",
      "Epoch:  3 Loss:  0.9579724243000619 \tacc:  92.5754060324826 \tsample_prob:  0.05\n",
      "Epoch:  4 Loss:  0.5302181895481369 \tacc:  95.78220858895705 \tsample_prob:  0.05\n",
      "Epoch:  5 Loss:  0.33177723672346326 \tacc:  97.74669774669775 \tsample_prob:  0.2\n",
      "Epoch:  6 Loss:  0.22643735915571223 \tacc:  97.9182156133829 \tsample_prob:  0.4\n",
      "Epoch:  7 Loss:  0.1629948131306198 \tacc:  98.27067669172932 \tsample_prob:  0.6\n",
      "Epoch:  8 Loss:  0.11937540788172493 \tacc:  99.47526236881559 \tsample_prob:  0.8\n",
      "Epoch:  9 Loss:  0.09096824365998057 \tacc:  99.04686258935664 \tsample_prob:  1.0\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    # sample_probability = (10 + epoch_index) / args.num_epochs\n",
    "    if epoch_index < 0.5 * args.num_epochs:\n",
    "        sample_probability = 0.05\n",
    "    else:\n",
    "        sample_probability = ( 2 * (epoch_index+1) - args.num_epochs) / args.num_epochs\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model_zero.train()\n",
    "    \n",
    "    for batch_ind, rus_batch, rsl_batch in batch_generator(vec_sentences_rus_train, vec_sentences_rsl_train, args.batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model_zero(rus_batch, rsl_batch, 0.0)\n",
    "        \n",
    "        loss = sequence_loss(y_pred, rsl_batch, args.mask_index)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss.item() - running_loss) / (batch_ind + 1)\n",
    "        acc_t = compute_accuracy(y_pred, rsl_batch, args.mask_index)\n",
    "        \n",
    "    print('Epoch: ', epoch_index, 'Loss: ', running_loss, '\\tacc: ', acc_t, '\\tsample_prob: ', sample_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "cellId": "o12hk3jg2vlxdasyy5n92e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_zero = model_zero.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "cellId": "gfcv50ggg7uovlo26t8j8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "regex = re.compile(r'(?:<bos>|<eos>.*)')\n",
    "def pred_vs_rsl(tensor, true_rsl, rus):\n",
    "    \n",
    "    tensor = tensor.argmax(2).tolist()\n",
    "    \n",
    "    pred_rsl = voc_rsl.index_to_text(tensor)\n",
    "    true_rsl = voc_rsl.index_to_text(true_rsl)\n",
    "    rus = voc_rus.index_to_text(rus)\n",
    "                   \n",
    "    f = lambda x: regex.sub('', \" \".join(x))\n",
    "                   \n",
    "    pred_rsl = [f(sentence) for sentence in pred_rsl]\n",
    "    true_rsl = [f(sentence) for sentence in true_rsl]\n",
    "    true_rus = [f(sentence) for sentence in rus]\n",
    "    \n",
    "    return pred_rsl, true_rsl, true_rus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "lm8yns4onmqdzfa8mq59yc"
   },
   "source": [
    "### Тестируем модель zero-shot\n",
    "\n",
    "на исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "cellId": "xnkpwczvohfhmi0t2sd1ch"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "for k, phrase in enum(test_gram_rus):\n",
    "    if phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "cellId": "hjgx4lvo5u5584tr9u3g5y"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "test_rsl_clean = [phrase for phrase in test_rsl if phrase is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "cellId": "uhkouzi51ebks4oyjokoxr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "for phrase in test_gram_rus:\n",
    "    if phrase is None:\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "cellId": "h9u83e6no1tj43awuahtg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "vec_sentences_rus_test = voc_rus.text_to_index(test_stem_rus)\n",
    "vec_gram_test = bin_gram.transform(test_gram_rus_clean)\n",
    "vec_sentences_rsl_test = voc_rsl.text_to_index(test_rsl_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "cellId": "x4glnfwjab907nbktuhif"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): Sizes of tensors must match except in dimension 1. Got 11532 and 1220 in dimension 0 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-f701f0ba4d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_sentences_rus_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_sentences_rsl_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, rus_sentances, rsl_sentence, sample_probability)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrus_sentances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsl_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mencoder_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrus_sentances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mdecoded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsl_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_state, init_h_state, rsl_sentence, sample_probability)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0my_input_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsl_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_input_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mh_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): Sizes of tensors must match except in dimension 1. Got 11532 and 1220 in dimension 0 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "y_pred = model_zero(torch.tensor(vec_sentences_rus_test), torch.tensor(vec_sentences_rsl_test), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellId": "v9pmjbpcau8lqy5nf2tvlo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "trans, truth_rsl, rus = pred_vs_rsl(y_pred,\n",
    "                                    torch.tensor(vec_sentences_rsl_test),\n",
    "                                    torch.tensor(vec_sentences_rus_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellId": "razknpqyam3t2750qkjzp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSL:   indx <nums> есть маленький 3ps эмоция готовый <dact> clf шуба.выбивать сапоги clf обувь.чистить санки готовый порядок эмоция скоро новый год \n",
      "TRANS:   indx <nums> есть маленький 3ps эмоция готовый <dact> clf шуба.выбивать сапоги clf обувь.чистить санки готовый порядок эмоция скоро новый год \n",
      "RUS:   там быть один маленький дед мороз который очень ждать новый год у он готовый вс рано остальной шуба выбивать сапог начищать и сани готовый \n",
      "\n",
      "\n",
      "RSL:   все дед.мороз неинтересно пусть думать что подарок дети <nums> 3ps маленький готово подарок коробка pl готово цвет свой pl \n",
      "TRANS:   все дед.мороз неинтересно пусть думать что подарок дети <nums> 3ps маленький готово подарок аппетит pl готово цвет свой pl \n",
      "RUS:   пока другой дед мороз быть неинтересно что можно приносить ребенок подарок маленький дед мороз уже давно лежать готовый завертывать в разноцветный бумага \n",
      "\n",
      "\n",
      "RSL:   это 3ps идеальный печь печенье <dact> вкус любой это 3ps уметь форма звезда ель разный pl \n",
      "TRANS:   это 3ps идеальный печь печенье <dact> вкус любой это 3ps уметь форма звезда ель разный pl \n",
      "RUS:   он замечательно п к печение любой форма например в вид зв здочка лочек \n",
      "\n",
      "\n",
      "RSL:   3ps подарок похоже готово clf <unk> готово это ждать все дед.мороз неинтересно опять2 не.хотеть ехать.поездом дети подарок pl назад домой не.хотеть похоже неинтересно поэтому думать обсудить пусть \n",
      "TRANS:   3ps подарок похоже готово clf самогон готово это ждать все дед.мороз неинтересно опять2 не.хотеть ехать.поездом дети подарок pl назад домой не.хотеть похоже неинтересно поэтому думать обсудить пусть \n",
      "RUS:   когда у маленький дед мороз весь подарок быть готовый остальной вс думать что снова надо будет ехать дарить ребенок подарок возвращаться домой что это очень скучно \n",
      "\n",
      "\n",
      "RSL:   <nums> дед.мороз главный все послушный2 говорить 3ps думать 3ps обращение маленький 3ps думать вместе не думать 2ps маленький пусть год мочь \n",
      "TRANS:   <nums> дед.мороз главный все послушный2 говорить 3ps думать 3ps обращение маленький 3ps думать вместе не думать 2ps маленький пусть год мочь \n",
      "RUS:   ты с мы нельзя говорить главный дед мороз который весь слушаться ты слишком маленький мочь в следущий год \n",
      "\n",
      "\n",
      "RSL:   маленький неинтересно готовый обида маленький все дед.мороз дразнить видеть 3ps маленький сани оставаться дети смеяться2 3ps дед.мороз маленький дразнить \n",
      "TRANS:   маленький неинтересно готовый обида маленький все дед.мороз дразнить видеть 3ps маленький сани оставаться дети смеяться2 3ps дед.мороз маленький дразнить \n",
      "RUS:   маленький дед мороз становиться очень обидно ведь он так готовиться а остальной смеяться и дразнить он из-за то что он маленький и сани у он нет \n",
      "\n",
      "\n",
      "RSL:   дед.мороз неинтересно 3ps дразнить 1ps неинтересно обида уезжать не мочь \n",
      "TRANS:   дед.мороз неинтересно 3ps дразнить 1ps неинтересно обида уезжать не мочь \n",
      "RUS:   дед мороз не обращать на они внимание он быть обидно только из-за то что он не мочь поехать с они \n",
      "\n",
      "\n",
      "RSL:   обида <unk> комната clf <unk> indx дед.мороз смешанный и уезжать вместе <dact> управление clf множество.движется потому что уезжать \n",
      "TRANS:   обида спорить комната clf \n",
      "RUS:   дед мороз убегать и закрываться в комната а весь остальной улетать на олений упряжка \n",
      "\n",
      "\n",
      "RSL:   сосредоточиться улица сосредоточиться знать clf <unk> звезда clf много.на.небе смотреть не.нужно indx дед.мороз вместе <dact> clf множество.движется управление поэтому не.нужно пусть \n",
      "TRANS:   сосредоточиться улица сосредоточиться знать clf самогон звезда clf много.на.небе смотреть не.нужно indx дед.мороз вместе <dact> clf множество.движется управление поэтому не.нужно пусть \n",
      "RUS:   он выходить на улица но <unk> вниз и не смотреть на небо потому что знать что там лететь дед мороз \n",
      "\n",
      "\n",
      "RSL:   слышать indx лес что разговаривать2 <dact> что indx был разговаривать2 медведь возмущаться заяц <dact> птица множество смешанный \n",
      "TRANS:   слышать indx лес что разговаривать2 <dact> что indx был разговаривать2 медведь возмущаться заяц <dact> птица множество смешанный \n",
      "RUS:   вдруг он <unk> в лес возмущать разговор зверь медведь заяц белок птица \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# lets have a look\n",
    "for i in range(10):\n",
    "    print(\"RSL: \", truth_rsl[i])\n",
    "    print(\"TRANS: \", trans[i])\n",
    "    print(\"RUS: \", rus[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellId": "ev7o3qgryilbf427cbee2h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Russian to RSL:  91.65096920794223\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import sacrebleu\n",
    "\n",
    "rus_rsl_bleu = sacrebleu.corpus_bleu(trans, [truth_rsl])\n",
    "print(\"--------------------------\")\n",
    "print(\"Russian to RSL: \", rus_rsl_bleu.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "5abbcba7-afa7-4dbe-8c28-c519ab09f6db",
  "notebookPath": "Notebooks/generate_outofexistent_nounsverbs.ipynb",
  "ydsNotebookPath": "Notebooks/generate_outofexistent_nounsverbs.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

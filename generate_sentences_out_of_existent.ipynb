{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "s2muf1hyw0b6v8b58jtv4s"
   },
   "source": [
    "# ~~~\n",
    "\n",
    "## Эта тетрадка посвящена замене слов в предложениях из существующих реально разметок\n",
    "\n",
    "Сюда надо перенести историю про одушевленность и переходность из первого файла по генерации.\n",
    "\n",
    "И сюда же надо будет грузить наречия трех видов, которые сделала Вика вот в этом файле: \n",
    "https://docs.google.com/spreadsheets/d/1nxMI0hznhXCROKxScsugj6-viQ8ibCBWLsgANdT_cmQ/edit#gid=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "opb16zdb2tryf1jmmesxe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pympi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "iowede438mmhuqkndn6sm"
   },
   "source": [
    "Сначала вспомогательный код для прочтения файлов с разметками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "yytmfntgfniarghsjk2228"
   },
   "outputs": [],
   "source": [
    "class DataToList:\n",
    "    # TODO: support files with two signers\n",
    "    def __init__(self, full_filename):\n",
    "        self.eaf = pympi.Elan.Eaf(full_filename)\n",
    "        if len(self.eaf.get_tier_names()) > 5:\n",
    "            self.second_person = True\n",
    "        else:\n",
    "            self.second_person = False\n",
    "\n",
    "    def read_data_rus(self):\n",
    "        eaf_form = self.eaf.get_annotation_data_for_tier(\"Перевод\")\n",
    "        rus_sentences = []\n",
    "\n",
    "        if eaf_form:\n",
    "            for sign in eaf_form:\n",
    "                # [0] is start time of annotation, [1] is end time\n",
    "                russian = (sign[0], sign[1], sign[2])\n",
    "                rus_sentences.append(russian)\n",
    "\n",
    "        # print(rus_sentences)\n",
    "        return rus_sentences\n",
    "\n",
    "    def read_data_rsl(self):\n",
    "        rus = self.read_data_rus()\n",
    "        rsl_sentences_raw = []\n",
    "        for sentence_rus in rus:\n",
    "            sentence = []\n",
    "            eaf_form_gloss = self.eaf.get_annotation_data_between_times(\"ПР-глосс\",\n",
    "                                                                   sentence_rus[0] - 2,\n",
    "                                                                   sentence_rus[1] + 2)\n",
    "            if eaf_form_gloss:\n",
    "                # 2 - не время, а текст аннотации\n",
    "                for word in eaf_form_gloss:\n",
    "                    new_word = word[2].lower()\n",
    "\n",
    "                    # проверить на дактиль и заменить его на <UNK> токен\n",
    "                    #if re.match(r'.(-.)+', new_word):\n",
    "                    #    sentence.append('<unk>')\n",
    "                    #    continue\n",
    "\n",
    "                    # проверить есть ли согласование\n",
    "                    ending = ''\n",
    "                    if ':' in new_word:\n",
    "                        if new_word[1:].lower().startswith('ps'):\n",
    "                            sentence.append(new_word[:4])\n",
    "                            new_word = new_word[4:]\n",
    "                        if new_word.lower().endswith('pl'):\n",
    "                            ending = ':pl'\n",
    "                            new_word = new_word[:-3]\n",
    "                        if new_word.lower().endswith('ps'):\n",
    "                            ending = new_word[-4:]\n",
    "                            new_word = new_word[:-4]\n",
    "                        sentence.append(new_word)\n",
    "                        if ending != '':\n",
    "                            sentence.append(ending)\n",
    "                    else:\n",
    "                        sentence.append(new_word)\n",
    "            else:\n",
    "                print(\"ERROR ERROR для предложения нет глосс ERROR ERROR\")\n",
    "            rsl_sentences_raw.append(sentence)\n",
    "\n",
    "        # этот разделитель токенов - костыль, чтобы можно было использовать токенизацию\n",
    "        # через пайплайн Field > TabularDataset > Iterator\n",
    "        # То есть РЖЯ уже токенизирован, но это придется сделать еще раз\n",
    "        rsl_sentences_raw = ['%>%'.join(s) for s in rsl_sentences_raw]\n",
    "        # print(rsl_sentences_raw)\n",
    "        return rsl_sentences_raw\n",
    "\n",
    "\n",
    "def read_elan_files(elan_file_path):\n",
    "    # Read elan files into two lists of sentences\n",
    "    rus_sentences, rsl_sentences = [], []\n",
    "    for dir_path, dir_names, filenames in os.walk(elan_file_path):\n",
    "        print(filenames)\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.eaf'):\n",
    "                full_filename = os.path.join(dir_path, filename)\n",
    "                rus = [sentence[2] for sentence in DataToList(full_filename).read_data_rus()]\n",
    "                rsl = DataToList(full_filename).read_data_rsl()\n",
    "                rus_sentences.extend(rus)\n",
    "                rsl_sentences.extend(rsl)\n",
    "    return rus_sentences, rsl_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ryfymx4ijr8j7hxdu8g4tc"
   },
   "outputs": [],
   "source": [
    "rus, rsl = read_elan_files(\"../translation/Разметки\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "lwmmxeormy85u3gy6zsldi"
   },
   "outputs": [],
   "source": [
    "print(len(rus), len(rsl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "1g72ua1hjixj8ldo1wuzz7q"
   },
   "source": [
    "Детур: сделаю txt файл из русских разметок, чтобы добавить их к нграммам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "fljcvvwbqulycym8ds4gjl"
   },
   "outputs": [],
   "source": [
    "with open('real_rus_translations.txt', 'w', encoding='utf-8') as f:\n",
    "    for sent in rus:\n",
    "        f.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "sa9ohx9vm8n4gbtdt3l7ns"
   },
   "source": [
    "Детур закончился"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "sn9tnw61bkm1eer245na3f"
   },
   "outputs": [],
   "source": [
    "split_rsl = [el.split(\"%>%\") for el in rsl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "02er9vmse9mmz1pa1xyw2e"
   },
   "outputs": [],
   "source": [
    "pprint(split_rsl[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xoz11z96mnf9sdaxlxv2b5"
   },
   "source": [
    "## Замена наречий\n",
    "\n",
    "Теперь попробоуем искать в предложениях на ржя наречия определенного типа и заменять их на наречия того же типа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0uwxiucwdayfmkhcvsikv"
   },
   "outputs": [],
   "source": [
    "time_adv = pd.read_csv(\"time_adverbs.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0b1tu4qnl36h2qebj2bb4pv"
   },
   "outputs": [],
   "source": [
    "time_adv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "41p1z854jrmmgv0ncvfbq"
   },
   "outputs": [],
   "source": [
    "for adverb in time_adv.text:\n",
    "    print(adverb.upper())\n",
    "    for k, sentence in enumerate(split_rsl):\n",
    "        if adverb in sentence:\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vlhjhw30bu8336oeh4ucfl"
   },
   "source": [
    "### Теперь можно генерировать фейковые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0qqp2r36aflwre5jo7i5u8"
   },
   "outputs": [],
   "source": [
    "with open(\"time_adverbs_fake_data.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for adverb in time_adv.text:\n",
    "        other = list(time_adv.text)\n",
    "        other.remove(adverb)\n",
    "        for k, sentence in enumerate(split_rsl):\n",
    "            if adverb in sentence:\n",
    "                for other_adverb in other:\n",
    "                    rus_new = re.sub(adverb, other_adverb, rus[k], flags=re.I)\n",
    "                    f.write(\" \".join([other_adverb if word == adverb else word for word in sentence]) + \"\\t\" +\\\n",
    "                            rus_new.capitalize() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ov4siirnoe85kl8phijnq6"
   },
   "source": [
    "Перейдем к наречиям места"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9ojog1shz4w0bla60ddme"
   },
   "outputs": [],
   "source": [
    "place_adv = pd.read_csv(\"place_adverbs.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "apnjazac2o5meqz2xbb73"
   },
   "outputs": [],
   "source": [
    "place_adv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "vuc5ji7mbeotsbyr2icf"
   },
   "outputs": [],
   "source": [
    "for adverb in place_adv.text:\n",
    "    print(adverb.upper())\n",
    "    for k, sentence in enumerate(split_rsl):\n",
    "        if adverb in sentence:\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ecu4ila9xxl0xctiq2io8l"
   },
   "outputs": [],
   "source": [
    "with open(\"place_adverbs_fake_data.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for adverb in place_adv.text:\n",
    "        other = list(place_adv.text)\n",
    "        other.remove(adverb)\n",
    "        for k, sentence in enumerate(split_rsl):\n",
    "            if adverb in sentence:\n",
    "                for other_adverb in other:\n",
    "                    rus_new = re.sub(adverb, other_adverb, rus[k], flags=re.I)\n",
    "                    f.write(\" \".join([other_adverb if word == adverb else word for word in sentence]) + \"\\t\" +\\\n",
    "                            rus_new.capitalize() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2u5qtjna2cq9bfzzpj5u5q"
   },
   "source": [
    "С наречиями места, если честно, получилась какая-то хрень.\n",
    "\n",
    "Я отсмотрела глазами этот файл и поняла, что это вроде как не наречия, а скорее предлоги, потому что они постоянно задают какое-то композициональное значение... Даже типа не попадают обычно в русский перевод. Если я права, то это могло бы быть очень интересно с лингвистической точки зрения, поэтому я соберу реальные предложения с этими предлогами в один файлик, чтобы потом вместе его посмотреть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "g660sriz4g59aienjxvhbw"
   },
   "outputs": [],
   "source": [
    "with open(\"place_adverbs_real_data.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for adverb in place_adv.text:\n",
    "        f.write(adverb.upper() + \"\\n\")\n",
    "        for k, sentence in enumerate(split_rsl):\n",
    "            if adverb in sentence:\n",
    "                f.write(\" \".join(sentence) + \"\\t\" + rus[k].capitalize() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5gxd2jk41bgwrd2f1dhwc"
   },
   "source": [
    "И наконец наречия образа действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "05os9yp8y14vuxsw4a5j8jn"
   },
   "outputs": [],
   "source": [
    "move_adv = pd.read_csv(\"move_adv.csv\", encoding=\"utf-8\")\n",
    "move_adv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "p14mw4gdaxfcfpiikx4cr"
   },
   "outputs": [],
   "source": [
    "for adverb in move_adv.text:\n",
    "    print(adverb.upper())\n",
    "    for k, sentence in enumerate(split_rsl):\n",
    "        if adverb in sentence:\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "610dlopo4hv4nemzeezema"
   },
   "outputs": [],
   "source": [
    "with open(\"move_adverbs_fake_data.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for adverb in move_adv.text:\n",
    "        other = list(move_adv.text)\n",
    "        other.remove(adverb)\n",
    "        for k, sentence in enumerate(split_rsl):\n",
    "            if adverb in sentence:\n",
    "                for other_adverb in other:\n",
    "                    rus_new = re.sub(adverb, other_adverb, rus[k], flags=re.I)\n",
    "                    f.write(\" \".join([other_adverb if word == adverb else word for word in sentence]) + \"\\t\" +\\\n",
    "                            rus_new.capitalize() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "794xcsaromrxnsr4a36z8d"
   },
   "source": [
    "## Замена существительных\n",
    "\n",
    "Теперь можно попробовать заменять субъекты, я беру именно субъекты, потому что там не надо париться с падежом и вероятность того, что предложение будет нормальным выше. Париться надо будет по поводу одушевленности и числа. А еще проверять, что там одно слово, а не два."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "4bt4whotoyw7khj1zkvng6"
   },
   "outputs": [],
   "source": [
    "words = pd.read_csv(\"animation_pos.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "93vy5zzw1o4fx0lebhfbl"
   },
   "outputs": [],
   "source": [
    "words.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "q8e8k41b2celvcx616lkkr"
   },
   "source": [
    "Положу в txt список одуш\\неодуш существительных и отсмотрю руками их, чтобы удалить явные ошибки и вхождения, где было несколько слов через запятую (их не стоит разделять на строки, потому что мало ли че там случилось в новой версии словаря)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "sqy1lf1mqeig3hamkvb8f"
   },
   "outputs": [],
   "source": [
    "act_column = []\n",
    "for i in range(len(words)):\n",
    "    if words['pos'][i] == 'S' or words['pos'][i] == 'SPRO':\n",
    "        # print(words['analysis'][i])\n",
    "        preanalysis = json.loads(words['analysis'][i].replace(\"'\", '\"'))\n",
    "        if preanalysis[0]['analysis']:\n",
    "            ## Надо брать не нулевое слово, а то у которого в gr тег S !!\n",
    "            if 'неод' in preanalysis[0]['analysis'][0]['gr']:\n",
    "                act_column.append('inan')\n",
    "            else:\n",
    "                act_column.append('anim')\n",
    "        else:\n",
    "             act_column.append('inan')\n",
    "    else:\n",
    "        act_column.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "m64blkz6luyx39efej9"
   },
   "outputs": [],
   "source": [
    "len(act_column) == len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "pw57qk2x5z8nmk4n932uq"
   },
   "outputs": [],
   "source": [
    "words['animacy'] = act_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "m5krr21jybwb3bord4u9r"
   },
   "outputs": [],
   "source": [
    "words[words['animacy'] == 'anim']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "arvney1hptm0r6i9kaaffy"
   },
   "outputs": [],
   "source": [
    "#with open(\"animate.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#    for word in words[words['animacy'] == 'anim']['text']:\n",
    "#        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cyh5vnqlb59fd526jz8wkr"
   },
   "outputs": [],
   "source": [
    "#with open(\"inanimate.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#    for word in words[words['animacy'] == 'inan']['text']:\n",
    "#        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "iqffrtezs4csk25z08bh3r"
   },
   "source": [
    "Отсмотрела глазами оба файла и почистила их. Теперь их можно заново подгрузить обновленными. А предыдущий код больше не запускать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mh0hx7ij2mifv2kza1296r"
   },
   "outputs": [],
   "source": [
    "with open(\"animate.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    anim_nouns = [line.strip('\\n') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cchpvd419qc2igmvgjnp2b"
   },
   "outputs": [],
   "source": [
    "anim_nouns[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7f43qk78j1405bwmbubc8f"
   },
   "source": [
    "Теперь можно загрузить разметки и начать там всё заменять. Но! загрузка разметок у нас с Сашей должна быть одинаковая, поэтому сначала я подгружу в свою папочку его код и сделаю на нем апдейты (мне не нужны никакие теги и лемматизация, только загрузка сырых данных). Я хочу брать именно Сашин код на данном этапе, потому что в моем не хватало второго человечка и второй руки, а у него всё красиво по классам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "3kft6epbhzelcg2z0hvc2"
   },
   "outputs": [],
   "source": [
    "rsl_tokens = [sent.split(\"%>%\") for sent in rsl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xnkiv27q67hsbsfler0b3i"
   },
   "source": [
    "Надо отобрать предложения с активным субъектом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0gei070l7nfgpguqajldmn"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "class TextStemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = Mystem(disambiguation=False, entire_input=False)\n",
    "        self.relex = re.compile(r'[,()=|]')\n",
    "        self.retext = re.compile(r'[^-а-яa-z0-9_]', re.I)\n",
    "\n",
    "    def _stem_gram(self, text):\n",
    "        text = self.retext.sub(' ', text)\n",
    "\n",
    "        def f(lem):\n",
    "            if lem['analysis']:\n",
    "                lem = lem['analysis'][0]\n",
    "                lex = lem['lex']\n",
    "                gr = self.relex.sub(' ', lem['gr']).split()\n",
    "            else:\n",
    "                lex = lem['text']\n",
    "                gr = []\n",
    "            return {'lex': lex, 'gram': list(set(gr))}\n",
    "\n",
    "        text = self.stemmer.analyze(text)\n",
    "        text = map(f, filter(lambda x: 'analysis' in x, text))\n",
    "\n",
    "        return list(text)\n",
    "\n",
    "    def _stem_simple(self, text):\n",
    "        text = self.retext.sub(' ', text)\n",
    "        text = self.stemmer.lemmatize(text)\n",
    "        return [{'lex': t, 'gram': []} for t in text]\n",
    "\n",
    "    def stem(self, text, gram=True):\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "\n",
    "        f = self._stem_simple if not gram else self._stem_gram\n",
    "        text = [f(t) for t in text]\n",
    "        lex = [[t['lex'] for t in sentence] for sentence in text]\n",
    "        gr = [[t['gram'] for t in sentence] for sentence in text]\n",
    "        return lex, gr\n",
    "\n",
    "    def __delete__(self, instance):\n",
    "        self.stemmer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "3y183iz4w4hb3s2sq6p1b"
   },
   "outputs": [],
   "source": [
    "st = TextStemmer()\n",
    "\n",
    "stem_sentences_rus, gram_sentences_rus = st.stem(rus[11], gram=True)\n",
    "\n",
    "print(stem_sentences_rus, gram_sentences_rus, rus[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jyww11norgkisgz3jbaq"
   },
   "outputs": [],
   "source": [
    "# это интересно, но лучше собирать всё в широкий датафрейм,\n",
    "# чтобы не потерять перевод на ржя\n",
    "selected_sents = defaultdict(list)\n",
    "for k, sentence in enumerate(rsl_tokens):\n",
    "    for noun in anim_nouns:\n",
    "        if noun in sentence:\n",
    "            selected_sents[noun].append(rus[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xag0aaskt5kisqk071nu"
   },
   "source": [
    "### Выделяем подлежащее\n",
    "\n",
    "Теперь мне в selected_sents надо выделить группы подлежащего через udpipe или mystem. А потом проверить, что выделенное подлежащее имеет в себе это самое существительное в именительном падеже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "aftz7iescxq4g8ue0b8t7n"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "u1jjtltmzx23r8378eke9"
   },
   "outputs": [],
   "source": [
    "!pip install ufal.udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0uah1u7la5oi2mjotl92nzr"
   },
   "outputs": [],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "39w0blb2rz3n70m3dsjav"
   },
   "outputs": [],
   "source": [
    "import ufal.udpipe\n",
    "from model import Model\n",
    "import conllu\n",
    "from nltk.parse import DependencyGraph\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5u13bte2z3o0ywhbr6x70gg"
   },
   "outputs": [],
   "source": [
    "model = Model('russian-syntagrus-ud-2.4-190531.udpipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "voy3il9cfrmq6q5a6m8go"
   },
   "outputs": [],
   "source": [
    "def get_conllu(model, text):\n",
    "    sentences = model.tokenize(text)\n",
    "    for s in sentences:\n",
    "        model.tag(s)\n",
    "        model.parse(s)\n",
    "    conllu_text = model.write(sentences, \"conllu\")\n",
    "    return conllu_text\n",
    "\n",
    "def get_dep_tree(text):\n",
    "    trees = []\n",
    "    for sent in text.split('\\n\\n'):\n",
    "        tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "        trees.append('\\n'.join(tree))\n",
    "    return trees\n",
    "\n",
    "def get_subtree(nodes, node):\n",
    "    if not nodes[node]['deps']:\n",
    "        return [node]\n",
    "    \n",
    "    else:\n",
    "        return [node] + [get_subtree(nodes, dep) for rel in nodes[node]['deps'] \n",
    "                         if rel != 'punct'  # пунктуацию доставать не будем\n",
    "                         for dep in nodes[node]['deps'][rel]]\n",
    "\n",
    "def flatten(l):\n",
    "    flat = []\n",
    "    for el in l:\n",
    "        if not isinstance(el, list):\n",
    "            flat.append(el)\n",
    "        else:\n",
    "            flat += flatten(el)\n",
    "    return flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "bsfc7af8zilugcglk6gbpc"
   },
   "outputs": [],
   "source": [
    "selected_sents['человек']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "u6v5kpq33pb4rn2q9y38t7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_subject(selected_sents, noun):\n",
    "    sents = defaultdict(list)\n",
    "    for sent in selected_sents[noun]:\n",
    "        print('=================================')\n",
    "        print(\"FULL SENTENCE   \", sent)\n",
    "        a = get_conllu(model, sent)\n",
    "        b = get_dep_tree(a)\n",
    "        d = DependencyGraph(b[0])\n",
    "        d.root = d.nodes[0]\n",
    "        #print(list(d.triples()))\n",
    "        for el in list(d.triples()):\n",
    "            if el[1] == 'nsubj':\n",
    "                nsubj = el[2][0]\n",
    "                #print(\"NSUBJ    \", nsubj)\n",
    "                d.nx_graph()\n",
    "                if nsubj in d.nx_labels.values():\n",
    "                    ind_subj = list(d.nx_labels.values()).index(nsubj) + 1\n",
    "                    subject = \" \".join([d.nodes[i]['word'] for i in sorted(flatten(get_subtree(d.nodes, ind_subj)))])\n",
    "                    print(\"SUBJECT   \", subject)\n",
    "                    sents[sent].append(subject)\n",
    "    return sents\n",
    "\n",
    "def save_sentences_and_subjects(sentences, noun):\n",
    "    with open(noun + \".csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qaho68ihjqfoii27wdgep"
   },
   "outputs": [],
   "source": [
    "noun = 'человек'\n",
    "extract_subject(selected_sents, noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2jize5pg78wtk00rkwwr7b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "zoji3kke87tykmkr70is"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "044b7745-b8c1-4aa4-b617-8fd9411bc91b",
  "notebookPath": "Notebooks/generate_sentences_out_of_existent.ipynb",
  "ydsNotebookPath": "Notebooks/generate_sentences_out_of_existent.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
